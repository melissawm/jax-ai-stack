
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Porting a PyTorch model to JAX &#8212; JAX AI Stack</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css?v=6644e6bb" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/css/custom.css?v=1dc24ef2" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=c1d4a9c3"></script>
    <script src="_static/doctools.js?v=9bcbadda"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'JAX_porting_PyTorch_model';</script>
    <link rel="icon" href="_static/favicon.png"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Train a miniGPT language model with JAX" href="JAX_for_LLM_pretraining.html" />
    <link rel="prev" title="JAX for PyTorch users" href="JAX_for_PyTorch_users.html" /> 
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>   
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/ai-stack-logo.svg" class="logo__image only-light" alt="JAX AI Stack - Home"/>
    <script>document.write(`<img src="_static/ai-stack-logo.svg" class="logo__image only-dark" alt="JAX AI Stack - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="index.html">
                    JAX AI Stack
                </a>
            </li>
        </ul>
        <ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="blog.html">JAX AI Stack Blog</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="install.html">Installing the stack</a></li>

<li class="toctree-l1 has-children"><a class="reference internal" href="getting_started.html">Getting started with JAX for ML</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="neural_net_basics.html">Part 1: JAX neural net basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="digits_vae.html">Part 2: Debug a variational autoencoder (VAE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="digits_diffusion_model.html">Part 3: Train a diffusion model for image generation</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="JAX_visualizing_models_metrics.html">Visualize JAX model metrics with TensorBoard</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="data_loaders.html">Introduction to Data Loaders</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="data_loaders_on_cpu_with_jax.html">Introduction to Data Loaders on CPU with JAX</a></li>
<li class="toctree-l2"><a class="reference internal" href="data_loaders_on_gpu_with_jax.html">Introduction to Data Loaders on GPU with JAX</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="pytorch_users.html">From PyTorch to JAX</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="JAX_for_PyTorch_users.html">JAX for PyTorch users</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Porting a PyTorch model to JAX</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Example applications</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="JAX_for_LLM_pretraining.html">Train a miniGPT language model with JAX</a></li>
<li class="toctree-l1"><a class="reference internal" href="JAX_basic_text_classification.html">Basic text classification with 1D CNN</a></li>
<li class="toctree-l1"><a class="reference internal" href="JAX_transformer_text_classification.html">Text classification with a transformer language model using JAX</a></li>
<li class="toctree-l1"><a class="reference internal" href="JAX_machine_translation.html">Machine Translation with encoder-decoder transformer model</a></li>
<li class="toctree-l1"><a class="reference internal" href="JAX_examples_image_segmentation.html">Image segmentation with UNETR model</a></li>
<li class="toctree-l1"><a class="reference internal" href="JAX_image_captioning.html">Image Captioning with Vision Transformer (ViT) model</a></li>
<li class="toctree-l1"><a class="reference internal" href="JAX_Vision_transformer.html">Train a Vision Transformer (ViT) for image classification with JAX</a></li>
<li class="toctree-l1"><a class="reference internal" href="JAX_time_series_classification.html">Time series classification with CNN</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Developer resources</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contribute to documentation</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/jax-ml/jax-ai-stack" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/JAX_porting_PyTorch_model.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Porting a PyTorch model to JAX</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#maxvit-pytorch-model-setup">MaxViT PyTorch model setup</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-s-architecture">Model’s architecture</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inference-on-data">Inference on data</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#port-maxvit-model-to-jax">Port MaxViT model to JAX</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conv2dnormactivation-implementation"><code class="docutils literal notranslate"><span class="pre">Conv2dNormActivation</span></code> implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#squeezeexcitation-implementation"><code class="docutils literal notranslate"><span class="pre">SqueezeExcitation</span></code> implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stochasticdepth-implementation"><code class="docutils literal notranslate"><span class="pre">StochasticDepth</span></code> implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mbconv-implementation"><code class="docutils literal notranslate"><span class="pre">MBConv</span></code> implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#relativepositionalmultiheadattention-implementation"><code class="docutils literal notranslate"><span class="pre">RelativePositionalMultiHeadAttention</span></code> implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#swapaxes-windowpartition-windowdepartition-implementations"><code class="docutils literal notranslate"><span class="pre">SwapAxes</span></code>, <code class="docutils literal notranslate"><span class="pre">WindowPartition</span></code>, <code class="docutils literal notranslate"><span class="pre">WindowDepartition</span></code> implementations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#partitionattentionlayer-implementation"><code class="docutils literal notranslate"><span class="pre">PartitionAttentionLayer</span></code> implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maxvitlayer-implementation"><code class="docutils literal notranslate"><span class="pre">MaxVitLayer</span></code> implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maxvitblock-implementation"><code class="docutils literal notranslate"><span class="pre">MaxVitBlock</span></code> implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maxvit-implementation"><code class="docutils literal notranslate"><span class="pre">MaxVit</span></code> implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#test-jax-implementation">Test JAX implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#check-flax-model">Check Flax model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-reading">Further reading</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                   <section class="tex2jax_ignore mathjax_ignore" id="porting-a-pytorch-model-to-jax">
<h1>Porting a PyTorch model to JAX<a class="headerlink" href="#porting-a-pytorch-model-to-jax" title="Link to this heading">#</a></h1>
<p><a class="reference external" href="https://colab.research.google.com/github/jax-ml/jax-ai-stack/blob/main/docs/source/JAX_porting_PyTorch_model.ipynb"><img alt="Open in Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></p>
<p><strong>Note: On Colab we recommend running this on a T4 GPU instance.  On Kaggle we recommend a T4x2 or P100 instance.</strong></p>
<p>In this tutorial we will learn how to port a PyTorch model to JAX and <a class="reference external" href="https://flax.readthedocs.io/en/latest/nnx_basics.html">Flax</a>. Flax provides an API very similar to the PyTorch <code class="docutils literal notranslate"><span class="pre">torch.nn</span></code> module and porting PyTorch models is rather straightforward. To install Flax, we can simply execute the following command: <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">-U</span> <span class="pre">flax</span> <span class="pre">treescope</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>-Uq<span class="w"> </span>flax<span class="w"> </span>treescope
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ <span class=" -Color -Color-Green">424.2/424.2 kB</span> <span class=" -Color -Color-Red">8.7 MB/s</span> eta <span class=" -Color -Color-Cyan">0:00:00</span>
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ <span class=" -Color -Color-Green">175.6/175.6 kB</span> <span class=" -Color -Color-Red">10.7 MB/s</span> eta <span class=" -Color -Color-Cyan">0:00:00</span>
?25h
</pre></div>
</div>
</div>
</div>
<p>Say we have a trained PyTorch computer-vision model to classify images that we would like to port to JAX. We will use <a class="reference external" href="https://pytorch.org/vision/stable/index.html"><code class="docutils literal notranslate"><span class="pre">TorchVision</span></code></a> to provide a <a class="reference external" href="https://pytorch.org/vision/stable/models/maxvit.html">MaxVit</a> model trained on ImageNet (MaxViT: Multi-Axis Vision Transformer, <a class="reference external" href="https://arxiv.org/abs/2204.01697">https://arxiv.org/abs/2204.01697</a>).</p>
<p>First, we set up the model using TorchVision and explore briefly the model’s architecture and the blocks we need to port. Next, we define equivalent blocks and the whole model using Flax. After that, we port the weights. Finally, we run some tests to ensure the correctness of the ported model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">jax</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">jax.numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">jnp</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">flax</span><span class="w"> </span><span class="kn">import</span> <span class="n">nnx</span>
</pre></div>
</div>
</div>
</div>
<section id="maxvit-pytorch-model-setup">
<h2>MaxViT PyTorch model setup<a class="headerlink" href="#maxvit-pytorch-model-setup" title="Link to this heading">#</a></h2>
<section id="model-s-architecture">
<h3>Model’s architecture<a class="headerlink" href="#model-s-architecture" title="Link to this heading">#</a></h3>
<p>The MaxVit model is <a class="reference external" href="https://github.com/pytorch/vision/blob/945bdad7523806b15d3740ce6ace2fced9ef9d3b/torchvision/models/maxvit.py#L568">implemented in TorchVision</a>. If we inspect the <a class="reference external" href="https://github.com/pytorch/vision/blob/945bdad7523806b15d3740ce6ace2fced9ef9d3b/torchvision/models/maxvit.py#L707-L712">forward pass</a> of the model, we can see that it contains three high-level parts:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/pytorch/vision/blob/945bdad7523806b15d3740ce6ace2fced9ef9d3b/torchvision/models/maxvit.py#L641-L655">stem</a>: a few convolutions, batchnorms, GELU activations.</p></li>
<li><p><a class="reference external" href="https://github.com/pytorch/vision/blob/945bdad7523806b15d3740ce6ace2fced9ef9d3b/torchvision/models/maxvit.py#L672-L692">blocks</a>: list of MaxViT blocks</p></li>
<li><p><a class="reference external" href="https://github.com/pytorch/vision/blob/945bdad7523806b15d3740ce6ace2fced9ef9d3b/torchvision/models/maxvit.py#L696-L703">classifier</a>: adaptive average pooling, few linear layers and Tanh activation.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torchvision.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">maxvit_t</span><span class="p">,</span> <span class="n">MaxVit_T_Weights</span>

<span class="n">torch_model</span> <span class="o">=</span> <span class="n">maxvit_t</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">MaxVit_T_Weights</span><span class="o">.</span><span class="n">IMAGENET1K_V1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.10/dist-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Downloading: &quot;https://download.pytorch.org/models/maxvit_t-bc5ab103.pth&quot; to /root/.cache/torch/hub/checkpoints/maxvit_t-bc5ab103.pth
100%|██████████| 119M/119M [00:02&lt;00:00, 53.9MB/s]
</pre></div>
</div>
</div>
</div>
<p>We can use <code class="docutils literal notranslate"><span class="pre">flax.nnx.display</span></code> to display the model’s architecture:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># nnx.display(torch_model)</span>
</pre></div>
</div>
</div>
</div>
<p>We can see that there are four MaxViT blocks in the model and each block contains:</p>
<ul class="simple">
<li><p>MaxViT layers: two layers for blocks 0, 1, 3 and five layers for the block 4</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">torch_model</span><span class="o">.</span><span class="n">blocks</span><span class="p">),</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">layers</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">torch_model</span><span class="o">.</span><span class="n">blocks</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(4, [2, 2, 5, 2])
</pre></div>
</div>
</div>
</div>
<p>A <a class="reference external" href="https://github.com/pytorch/vision/blob/945bdad7523806b15d3740ce6ace2fced9ef9d3b/torchvision/models/maxvit.py#L386">MaxViT layer</a> is composed of: <a class="reference external" href="https://github.com/pytorch/vision/blob/945bdad7523806b15d3740ce6ace2fced9ef9d3b/torchvision/models/maxvit.py#L53"><code class="docutils literal notranslate"><span class="pre">MBConv</span></code></a>, <code class="docutils literal notranslate"><span class="pre">window_attention</span></code> as <a class="reference external" href="https://github.com/pytorch/vision/blob/945bdad7523806b15d3740ce6ace2fced9ef9d3b/torchvision/models/maxvit.py#L282"><code class="docutils literal notranslate"><span class="pre">PartitionAttentionLayer</span></code></a> and <code class="docutils literal notranslate"><span class="pre">grid_attention</span></code> as <code class="docutils literal notranslate"><span class="pre">PartitionAttentionLayer</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">[[</span><span class="n">mod</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="k">for</span> <span class="n">mod</span> <span class="ow">in</span> <span class="n">maxvit_layer</span><span class="o">.</span><span class="n">layers</span><span class="p">]</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">torch_model</span><span class="o">.</span><span class="n">blocks</span> <span class="k">for</span> <span class="n">maxvit_layer</span> <span class="ow">in</span> <span class="n">b</span><span class="o">.</span><span class="n">layers</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[&#39;MBConv&#39;, &#39;PartitionAttentionLayer&#39;, &#39;PartitionAttentionLayer&#39;],
 [&#39;MBConv&#39;, &#39;PartitionAttentionLayer&#39;, &#39;PartitionAttentionLayer&#39;],
 [&#39;MBConv&#39;, &#39;PartitionAttentionLayer&#39;, &#39;PartitionAttentionLayer&#39;],
 [&#39;MBConv&#39;, &#39;PartitionAttentionLayer&#39;, &#39;PartitionAttentionLayer&#39;],
 [&#39;MBConv&#39;, &#39;PartitionAttentionLayer&#39;, &#39;PartitionAttentionLayer&#39;],
 [&#39;MBConv&#39;, &#39;PartitionAttentionLayer&#39;, &#39;PartitionAttentionLayer&#39;],
 [&#39;MBConv&#39;, &#39;PartitionAttentionLayer&#39;, &#39;PartitionAttentionLayer&#39;],
 [&#39;MBConv&#39;, &#39;PartitionAttentionLayer&#39;, &#39;PartitionAttentionLayer&#39;],
 [&#39;MBConv&#39;, &#39;PartitionAttentionLayer&#39;, &#39;PartitionAttentionLayer&#39;],
 [&#39;MBConv&#39;, &#39;PartitionAttentionLayer&#39;, &#39;PartitionAttentionLayer&#39;],
 [&#39;MBConv&#39;, &#39;PartitionAttentionLayer&#39;, &#39;PartitionAttentionLayer&#39;]]
</pre></div>
</div>
</div>
</div>
</section>
<section id="inference-on-data">
<h3>Inference on data<a class="headerlink" href="#inference-on-data" title="Link to this heading">#</a></h3>
<p>Let’s check the model on dummy input and on a real image</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>


<span class="n">torch_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">torch_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (2, 1000)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([2, 1000])
</pre></div>
</div>
</div>
</div>
<p>We can download an image of a Pembroke Corgy dog from <a class="reference external" href="https://github.com/pytorch/vision/blob/main/gallery/assets/dog1.jpg?raw=true">TorchVision’s gallery</a> together with <a class="reference external" href="https://raw.githubusercontent.com/pytorch/vision/refs/heads/main/gallery/assets/imagenet_class_index.json">ImageNet classes dictionary</a>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="k">bash</span>
if [ -f &quot;dog1.jpg&quot; ]; then
  echo &quot;dog1.jpg already exists.&quot;
else
  wget -nv &quot;https://github.com/pytorch/vision/blob/main/gallery/assets/dog1.jpg?raw=true&quot; -O dog1.jpg
fi
if [ -f &quot;imagenet_class_index.json&quot; ]; then
  echo &quot;imagenet_class_index.json already exists.&quot;
else
  wget -nv &quot;https://raw.githubusercontent.com/pytorch/vision/refs/heads/main/gallery/assets/imagenet_class_index.json&quot; -O imagenet_class_index.json
fi
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2025-01-15 21:10:00 URL:https://raw.githubusercontent.com/pytorch/vision/refs/heads/main/gallery/assets/dog1.jpg [97422/97422] -&gt; &quot;dog1.jpg&quot; [1]
2025-01-15 21:10:01 URL:https://raw.githubusercontent.com/pytorch/vision/refs/heads/main/gallery/assets/imagenet_class_index.json [35364/35364] -&gt; &quot;imagenet_class_index.json&quot; [1]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">json</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchvision.io</span><span class="w"> </span><span class="kn">import</span> <span class="n">read_image</span>


<span class="n">preprocess</span> <span class="o">=</span> <span class="n">MaxVit_T_Weights</span><span class="o">.</span><span class="n">IMAGENET1K_V1</span><span class="o">.</span><span class="n">transforms</span><span class="p">()</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;imagenet_class_index.json&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">labels_file</span><span class="p">:</span>
  <span class="n">labels</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">labels_file</span><span class="p">)</span>


<span class="n">dog1</span> <span class="o">=</span> <span class="n">read_image</span><span class="p">(</span><span class="s2">&quot;dog1.jpg&quot;</span><span class="p">)</span>
<span class="n">tensor</span> <span class="o">=</span> <span class="n">preprocess</span><span class="p">(</span><span class="n">dog1</span><span class="p">)</span>

<span class="n">torch_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
  <span class="n">output</span> <span class="o">=</span> <span class="n">torch_model</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

<span class="n">class_id</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Prediction for the Dog: </span><span class="si">{</span><span class="n">labels</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">class_id</span><span class="p">)]</span><span class="si">}</span><span class="s2">, score: </span><span class="si">{</span><span class="n">output</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">class_id</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">labels</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">class_id</span><span class="p">)]</span><span class="si">}</span><span class="se">\n</span><span class="s2">Score: </span><span class="si">{</span><span class="n">output</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">class_id</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">dog1</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Prediction for the Dog: [&#39;n02113023&#39;, &#39;Pembroke&#39;], score: 0.7800846099853516
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.image.AxesImage at 0x7ae7b379cf70&gt;
</pre></div>
</div>
<img alt="_images/0672adfd0b01f6abf0246db2af65b82a66cd6c01069bf364b4efc23b60ae50d7.png" src="_images/0672adfd0b01f6abf0246db2af65b82a66cd6c01069bf364b4efc23b60ae50d7.png" />
</div>
</div>
</section>
</section>
<section id="port-maxvit-model-to-jax">
<h2>Port MaxViT model to JAX<a class="headerlink" href="#port-maxvit-model-to-jax" title="Link to this heading">#</a></h2>
<p>To port the <a class="reference external" href="https://github.com/pytorch/vision/blob/945bdad7523806b15d3740ce6ace2fced9ef9d3b/torchvision/models/maxvit.py#L568">PyTorch implementation of the MaxVit model</a> in JAX using the Flax module, we will implement the following required modules:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">MaxViT</span></code></p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">MaxVitBlock</span></code></p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">MaxVitLayer</span></code></p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">MBConv</span></code></p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">Conv2dNormActivation</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">SqueezeExcitation</span></code></p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">PartitionAttentionLayer</span></code></p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">RelativePositionalMultiHeadAttention</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">WindowDepartition</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">WindowPartition</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">SwapAxes</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">StochasticDepth</span></code></p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>The Flax NNX module is very similar to PyTorch <code class="docutils literal notranslate"><span class="pre">torch.nn</span></code> module and we can map the following modules between PyTorch and Flax:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">nn.Sequential</span></code> and <code class="docutils literal notranslate"><span class="pre">nn.ModuleList</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">nnx.Sequential</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">nn.Linear</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">nnx.Linear</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">nn.Conv2d</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">nnx.Conv</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">nn.BatchNorm2d</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">nnx.BatchNorm</span></code></p></li>
<li><p>Activations like <code class="docutils literal notranslate"><span class="pre">nn.ReLU</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">nnx.relu</span></code></p></li>
<li><p>Pooling layers like <code class="docutils literal notranslate"><span class="pre">nn.AvgPool2d(...)</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">lambda</span> <span class="pre">x:</span> <span class="pre">nnx.avg_pool(x,</span> <span class="pre">...)</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">nn.AdaptiveAvgPool2d(1)</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">lambda</span> <span class="pre">x:</span> <span class="pre">nnx.avg_pool(x,</span> <span class="pre">(x.shape[1],</span> <span class="pre">x.shape[2]))</span></code>, x is in NHWC format</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">nn.Flatten()</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">lambda</span> <span class="pre">x:</span> <span class="pre">x.reshape(x.shape[0],</span> <span class="pre">-1)</span></code></p></li>
</ul>
<p>If the PyTorch model defines a learnable parameter and a buffer:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Model</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">...</span><span class="p">):</span>
        <span class="o">...</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
</pre></div>
</div>
<p>an equivalent code in Flax would be</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Buffer</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Variable</span><span class="p">):</span>
    <span class="k">pass</span>


<span class="k">class</span><span class="w"> </span><span class="nc">Model</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">...</span><span class="p">):</span>
        <span class="o">...</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Param</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">10</span><span class="p">,)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">Buffer</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
</pre></div>
</div>
<p>To inspect NNX module’s learnable parameters and buffers, we can use <code class="docutils literal notranslate"><span class="pre">nnx.state</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nnx_module</span> <span class="o">=</span> <span class="o">...</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">nnx</span><span class="o">.</span><span class="n">state</span><span class="p">(</span><span class="n">nnx_module</span><span class="p">,</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Param</span><span class="p">)</span><span class="o">.</span><span class="n">flat_state</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="n">k</span><span class="p">,</span>
        <span class="n">v</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="k">if</span> <span class="n">v</span><span class="o">.</span><span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="p">)</span>

<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">nnx</span><span class="o">.</span><span class="n">state</span><span class="p">(</span><span class="n">nnx_module</span><span class="p">,</span> <span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">BatchStat</span><span class="p">,</span> <span class="n">Buffer</span><span class="p">))</span><span class="o">.</span><span class="n">flat_state</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="n">k</span><span class="p">,</span>
        <span class="n">v</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="k">if</span> <span class="n">v</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="s2">&quot;float32&quot;</span> <span class="k">else</span> <span class="n">v</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="p">)</span>
</pre></div>
</div>
<p>The equivalent PyTorch code is:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">torch_module</span> <span class="o">=</span> <span class="o">...</span>

<span class="k">for</span> <span class="n">m</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">torch_module</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>

<span class="k">for</span> <span class="n">m</span><span class="p">,</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">torch_module</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="n">m</span><span class="p">,</span>
        <span class="n">b</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="k">if</span> <span class="n">b</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span> <span class="k">else</span> <span class="n">b</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="p">)</span>
</pre></div>
</div>
<p>Please note some differences between <code class="docutils literal notranslate"><span class="pre">torch.nn</span></code> and Flax when porting models:</p>
<ul class="simple">
<li><p>We should pass <code class="docutils literal notranslate"><span class="pre">rngs</span></code> to all NNX modules with parameters: e.g. <code class="docutils literal notranslate"><span class="pre">nnx.Linear(...,</span> <span class="pre">rngs=nnx.Rngs(0))</span></code></p></li>
<li><p>For a 2D convolution:</p>
<ul>
<li><p>In Flax, we need to explicitly define <code class="docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="docutils literal notranslate"><span class="pre">strides</span></code> as two ints tuples, e.g. <code class="docutils literal notranslate"><span class="pre">(3,</span> <span class="pre">3)</span></code></p></li>
<li><p>If PyTorch code defines <code class="docutils literal notranslate"><span class="pre">padding</span></code> as integer, e.g. 2, in Flax it should be explicitly defined as a tuple of two ints per dimension, i.e. <code class="docutils literal notranslate"><span class="pre">((2,</span> <span class="pre">2),</span> <span class="pre">(2,</span> <span class="pre">2))</span></code>.</p></li>
</ul>
</li>
<li><p>For a batch normalization: <code class="docutils literal notranslate"><span class="pre">momentum</span></code> value in <code class="docutils literal notranslate"><span class="pre">torch.nn</span></code> should be defined as <code class="docutils literal notranslate"><span class="pre">1.0</span> <span class="pre">-</span> <span class="pre">momentum</span></code> in Flax.</p></li>
<li><p>4D input arrays in Flax should be in NHWC format, i.e. of shape (N, H, W, C) compared to NCHW format (or (N, C, H, W) shape) in PyTorch.</p></li>
</ul>
<p>Below we implement one by one all the modules from the above list and add simple forward pass checks.
Let’s first implement equivalent of <code class="docutils literal notranslate"><span class="pre">nn.Identity</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Identity</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<section id="conv2dnormactivation-implementation">
<h3><code class="docutils literal notranslate"><span class="pre">Conv2dNormActivation</span></code> implementation<a class="headerlink" href="#conv2dnormactivation-implementation" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://github.com/pytorch/vision/blob/945bdad7523806b15d3740ce6ace2fced9ef9d3b/torchvision/ops/misc.py#L125">PyTorch source implementation</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">flax</span><span class="w"> </span><span class="kn">import</span> <span class="n">nnx</span>


<span class="k">class</span><span class="w"> </span><span class="nc">Conv2dNormActivation</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Sequential</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">norm_layer</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">,</span>
        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span>
        <span class="n">dilation</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">rngs</span><span class="p">:</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span> <span class="o">=</span> <span class="n">out_channels</span>

        <span class="k">if</span> <span class="n">padding</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">padding</span> <span class="o">=</span> <span class="p">(</span><span class="n">kernel_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">dilation</span>
        <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">bias</span> <span class="o">=</span> <span class="n">norm_layer</span> <span class="ow">is</span> <span class="kc">None</span>

        <span class="c1"># sequence integer pairs that give the padding to apply before</span>
        <span class="c1"># and after each spatial dimension</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="p">((</span><span class="n">padding</span><span class="p">,</span> <span class="n">padding</span><span class="p">),</span> <span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="n">padding</span><span class="p">))</span>

        <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">nnx</span><span class="o">.</span><span class="n">Conv</span><span class="p">(</span>
                <span class="n">in_channels</span><span class="p">,</span>
                <span class="n">out_channels</span><span class="p">,</span>
                <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">),</span>
                <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="n">stride</span><span class="p">),</span>
                <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
                <span class="n">kernel_dilation</span><span class="o">=</span><span class="p">(</span><span class="n">dilation</span><span class="p">,</span> <span class="n">dilation</span><span class="p">),</span>
                <span class="n">feature_group_count</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span>
                <span class="n">use_bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
                <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="p">]</span>

        <span class="k">if</span> <span class="n">norm_layer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">norm_layer</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">activation_layer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">activation_layer</span><span class="p">)</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">32</span><span class="p">))</span>
<span class="n">mod</span> <span class="o">=</span> <span class="n">Conv2dNormActivation</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">mod</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(4, 14, 14, 64)
</pre></div>
</div>
</div>
</div>
</section>
<section id="squeezeexcitation-implementation">
<h3><code class="docutils literal notranslate"><span class="pre">SqueezeExcitation</span></code> implementation<a class="headerlink" href="#squeezeexcitation-implementation" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://github.com/pytorch/vision/blob/945bdad7523806b15d3740ce6ace2fced9ef9d3b/torchvision/ops/misc.py#L224">PyTorch source implementation</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">SqueezeExcitation</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">squeeze_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">activation</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span>
        <span class="n">scale_activation</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">,</span>
        <span class="n">rngs</span><span class="p">:</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">avgpool</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">avg_pool</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Conv</span><span class="p">(</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">squeeze_channels</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Conv</span><span class="p">(</span><span class="n">squeeze_channels</span><span class="p">,</span> <span class="n">input_channels</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">activation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale_activation</span> <span class="o">=</span> <span class="n">scale_activation</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_scale</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">:</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">avgpool</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">scale</span><span class="p">)</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">scale</span><span class="p">)</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">scale</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_activation</span><span class="p">(</span><span class="n">scale</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">:</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scale</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">32</span><span class="p">))</span>
<span class="n">mod</span> <span class="o">=</span> <span class="n">SqueezeExcitation</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">mod</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(4, 28, 28, 32)
</pre></div>
</div>
</div>
</div>
</section>
<section id="stochasticdepth-implementation">
<h3><code class="docutils literal notranslate"><span class="pre">StochasticDepth</span></code> implementation<a class="headerlink" href="#stochasticdepth-implementation" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://github.com/pytorch/vision/blob/945bdad7523806b15d3740ce6ace2fced9ef9d3b/torchvision/ops/stochastic_depth.py#L50">PyTorch source implementation</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">stochastic_depth</span><span class="p">(</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
    <span class="n">p</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
    <span class="n">mode</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">deterministic</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">rngs</span><span class="p">:</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">p</span> <span class="o">&lt;</span> <span class="mf">0.0</span> <span class="ow">or</span> <span class="n">p</span> <span class="o">&gt;</span> <span class="mf">1.0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;drop probability has to be between 0 and 1, but got </span><span class="si">{</span><span class="n">p</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;batch&quot;</span><span class="p">,</span> <span class="s2">&quot;row&quot;</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;mode has to be either &#39;batch&#39; or &#39;row&#39;, but got </span><span class="si">{</span><span class="n">mode</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">deterministic</span> <span class="ow">or</span> <span class="n">p</span> <span class="o">==</span> <span class="mf">0.0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="n">survival_rate</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">p</span>
    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;row&quot;</span><span class="p">:</span>
        <span class="n">size</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">size</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span>

    <span class="n">noise</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">bernoulli</span><span class="p">(</span>
        <span class="n">rngs</span><span class="o">.</span><span class="n">dropout</span><span class="p">(),</span> <span class="n">p</span><span class="o">=</span><span class="n">survival_rate</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">size</span>
    <span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">survival_rate</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">:</span>
        <span class="n">noise</span> <span class="o">=</span> <span class="n">noise</span> <span class="o">/</span> <span class="n">survival_rate</span>

    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">noise</span>


<span class="k">class</span><span class="w"> </span><span class="nc">StochasticDepth</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">p</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">mode</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">rngs</span><span class="p">:</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">=</span> <span class="n">p</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">deterministic</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rngs</span> <span class="o">=</span> <span class="n">rngs</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">stochastic_depth</span><span class="p">(</span>
            <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">p</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">deterministic</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">rngs</span>
        <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">32</span><span class="p">))</span>
<span class="n">mod</span> <span class="o">=</span> <span class="n">StochasticDepth</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="s2">&quot;row&quot;</span><span class="p">)</span>

<span class="n">mod</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">mod</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>

<span class="n">mod</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">mod</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">y</span> <span class="o">!=</span> <span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="mbconv-implementation">
<h3><code class="docutils literal notranslate"><span class="pre">MBConv</span></code> implementation<a class="headerlink" href="#mbconv-implementation" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://github.com/pytorch/vision/blob/945bdad7523806b15d3740ce6ace2fced9ef9d3b/torchvision/models/maxvit.py#L53">PyTorch source implementation</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">MBConv</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">expansion_ratio</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">squeeze_ratio</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">norm_layer</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">],</span>
        <span class="n">p_stochastic_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="n">rngs</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
    <span class="p">):</span>
        <span class="n">should_proj</span> <span class="o">=</span> <span class="n">stride</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">in_channels</span> <span class="o">!=</span> <span class="n">out_channels</span>
        <span class="k">if</span> <span class="n">should_proj</span><span class="p">:</span>
            <span class="n">proj</span> <span class="o">=</span> <span class="p">[</span><span class="n">nnx</span><span class="o">.</span><span class="n">Conv</span><span class="p">(</span>
                <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span>
            <span class="p">)]</span>
            <span class="k">if</span> <span class="n">stride</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                <span class="n">padding</span> <span class="o">=</span> <span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
                <span class="n">proj</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">nnx</span><span class="o">.</span><span class="n">avg_pool</span><span class="p">(</span>
                        <span class="n">x</span><span class="p">,</span> <span class="n">window_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="n">stride</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="n">padding</span>
                    <span class="p">)</span>
                <span class="p">]</span> <span class="o">+</span> <span class="n">proj</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">proj</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">Identity</span><span class="p">()</span>

        <span class="n">mid_channels</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">out_channels</span> <span class="o">*</span> <span class="n">expansion_ratio</span><span class="p">)</span>
        <span class="n">sqz_channels</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">out_channels</span> <span class="o">*</span> <span class="n">squeeze_ratio</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">p_stochastic_dropout</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">stochastic_depth</span> <span class="o">=</span> <span class="n">StochasticDepth</span><span class="p">(</span><span class="n">p_stochastic_dropout</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;row&quot;</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">stochastic_depth</span> <span class="o">=</span> <span class="n">Identity</span><span class="p">()</span>

        <span class="n">_layers</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">norm_layer</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">),</span>  <span class="c1"># pre_norm</span>
            <span class="n">Conv2dNormActivation</span><span class="p">(</span>    <span class="c1"># conv_a</span>
                <span class="n">in_channels</span><span class="p">,</span>
                <span class="n">mid_channels</span><span class="p">,</span>
                <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
                <span class="n">norm_layer</span><span class="o">=</span><span class="n">norm_layer</span><span class="p">,</span>
                <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="n">Conv2dNormActivation</span><span class="p">(</span>  <span class="c1"># conv_b</span>
                <span class="n">mid_channels</span><span class="p">,</span>
                <span class="n">mid_channels</span><span class="p">,</span>
                <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
                <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
                <span class="n">norm_layer</span><span class="o">=</span><span class="n">norm_layer</span><span class="p">,</span>
                <span class="n">groups</span><span class="o">=</span><span class="n">mid_channels</span><span class="p">,</span>
                <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="n">SqueezeExcitation</span><span class="p">(</span>  <span class="c1"># squeeze_excitation</span>
                <span class="n">mid_channels</span><span class="p">,</span> <span class="n">sqz_channels</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">nnx</span><span class="o">.</span><span class="n">silu</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span>
            <span class="p">),</span>
            <span class="n">nnx</span><span class="o">.</span><span class="n">Conv</span><span class="p">(</span>  <span class="c1"># conv_c</span>
                <span class="n">mid_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span>
            <span class="p">)</span>
        <span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">_layers</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">:</span>
        <span class="n">res</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stochastic_depth</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">res</span> <span class="o">+</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">functools</span><span class="w"> </span><span class="kn">import</span> <span class="n">partial</span>

<span class="n">norm_layer</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.99</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">32</span><span class="p">))</span>
<span class="n">mod</span> <span class="o">=</span> <span class="n">MBConv</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">activation_layer</span><span class="o">=</span><span class="n">nnx</span><span class="o">.</span><span class="n">gelu</span><span class="p">,</span> <span class="n">norm_layer</span><span class="o">=</span><span class="n">norm_layer</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">mod</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(4, 14, 14, 64)
</pre></div>
</div>
</div>
</div>
</section>
<section id="relativepositionalmultiheadattention-implementation">
<h3><code class="docutils literal notranslate"><span class="pre">RelativePositionalMultiHeadAttention</span></code> implementation<a class="headerlink" href="#relativepositionalmultiheadattention-implementation" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://github.com/pytorch/vision/blob/945bdad7523806b15d3740ce6ace2fced9ef9d3b/torchvision/models/maxvit.py#L140">PyTorch source implementation</a>. First we reimplement a helper function <code class="docutils literal notranslate"><span class="pre">_get_relative_position_index</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_get_relative_position_index</span><span class="p">(</span><span class="n">height</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">width</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">:</span>
    <span class="c1"># PyTorch code:</span>
    <span class="c1"># coords = torch.stack(torch.meshgrid([torch.arange(height), torch.arange(width)]))</span>

    <span class="n">coords</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span>
        <span class="n">jnp</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">height</span><span class="p">),</span> <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">width</span><span class="p">)],</span> <span class="n">indexing</span><span class="o">=</span><span class="s2">&quot;ij&quot;</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="c1"># PyTorch code: coords_flat = torch.flatten(coords, 1)</span>
    <span class="n">coords_flat</span> <span class="o">=</span> <span class="n">coords</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">coords</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">relative_coords</span> <span class="o">=</span> <span class="n">coords_flat</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">-</span> <span class="n">coords_flat</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">relative_coords</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">permute_dims</span><span class="p">(</span><span class="n">relative_coords</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>

    <span class="c1"># PyTorch code:</span>
    <span class="c1"># relative_coords[:, :, 0] += height - 1</span>
    <span class="c1"># relative_coords[:, :, 1] += width - 1</span>
    <span class="c1"># relative_coords[:, :, 0] *= 2 * width - 1</span>
    <span class="n">relative_coords</span> <span class="o">=</span> <span class="n">relative_coords</span> <span class="o">+</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">((</span><span class="n">height</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">width</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">relative_coords</span> <span class="o">=</span> <span class="n">relative_coords</span> <span class="o">*</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">((</span><span class="mi">2</span> <span class="o">*</span> <span class="n">width</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">relative_coords</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let us check our implementation against PyTorch implementation:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torchvision.models.maxvit</span><span class="w"> </span><span class="kn">import</span> <span class="n">_get_relative_position_index</span> <span class="k">as</span> <span class="n">pytorch_get_relative_position_index</span>


<span class="n">output</span> <span class="o">=</span> <span class="n">_get_relative_position_index</span><span class="p">(</span><span class="mi">13</span><span class="p">,</span> <span class="mi">12</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span> <span class="n">pytorch_get_relative_position_index</span><span class="p">(</span><span class="mi">13</span><span class="p">,</span> <span class="mi">12</span><span class="p">)</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">output</span> <span class="o">==</span> <span class="n">jnp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Next, we can port <code class="docutils literal notranslate"><span class="pre">RelativePositionalMultiHeadAttention</span></code> module which a learnable parameter and a buffer:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">math</span>


<span class="k">class</span><span class="w"> </span><span class="nc">Buffer</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Variable</span><span class="p">):</span>
    <span class="k">pass</span>


<span class="k">class</span><span class="w"> </span><span class="nc">RelativePositionalMultiHeadAttention</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">feat_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">head_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">max_seq_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">rngs</span><span class="p">:</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">feat_dim</span> <span class="o">%</span> <span class="n">head_dim</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;feat_dim: </span><span class="si">{</span><span class="n">feat_dim</span><span class="si">}</span><span class="s2"> must be divisible by head_dim: </span><span class="si">{</span><span class="n">head_dim</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">feat_dim</span> <span class="o">//</span> <span class="n">head_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">head_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">max_seq_len</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_seq_len</span> <span class="o">=</span> <span class="n">max_seq_len</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">to_qkv</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">feat_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale_factor</span> <span class="o">=</span> <span class="n">feat_dim</span><span class="o">**-</span><span class="mf">0.5</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">merge</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">feat_dim</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">relative_position_index</span> <span class="o">=</span> <span class="n">Buffer</span><span class="p">(</span><span class="n">_get_relative_position_index</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">))</span>

        <span class="c1"># initialize with truncated normal bias</span>
        <span class="n">initializer</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">(</span><span class="n">stddev</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="p">((</span><span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relative_position_bias_table</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Param</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="n">rngs</span><span class="o">.</span><span class="n">params</span><span class="p">(),</span> <span class="n">shape</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_relative_positional_bias</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">:</span>
        <span class="n">bias_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relative_position_index</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
        <span class="n">relative_bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relative_position_bias_table</span><span class="p">[</span><span class="n">bias_index</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">relative_bias</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">permute_dims</span><span class="p">(</span><span class="n">relative_bias</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">relative_bias</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">:</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">G</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">H</span><span class="p">,</span> <span class="n">DH</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span>

        <span class="n">qkv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_qkv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">qkv</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">permute_dims</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">B</span><span class="p">,</span> <span class="n">G</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">DH</span><span class="p">)),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">permute_dims</span><span class="p">(</span><span class="n">k</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">B</span><span class="p">,</span> <span class="n">G</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">DH</span><span class="p">)),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">permute_dims</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">B</span><span class="p">,</span> <span class="n">G</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">DH</span><span class="p">)),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

        <span class="n">k</span> <span class="o">=</span> <span class="n">k</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_factor</span>

        <span class="n">dot_prod</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;B G H I D, B G H J D -&gt; B G H I J&quot;</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
        <span class="n">pos_bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_relative_positional_bias</span><span class="p">()</span>

        <span class="n">dot_prod</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dot_prod</span> <span class="o">+</span> <span class="n">pos_bias</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;B G H I J, B G H J D -&gt; B G H I D&quot;</span><span class="p">,</span> <span class="n">dot_prod</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">permute_dims</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">B</span><span class="p">,</span> <span class="n">G</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">D</span><span class="p">))</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">49</span><span class="p">,</span> <span class="mi">64</span><span class="p">))</span>

<span class="n">mod</span> <span class="o">=</span> <span class="n">RelativePositionalMultiHeadAttention</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">49</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">mod</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(4, 32, 49, 64)
</pre></div>
</div>
</div>
</div>
</section>
<section id="swapaxes-windowpartition-windowdepartition-implementations">
<h3><code class="docutils literal notranslate"><span class="pre">SwapAxes</span></code>, <code class="docutils literal notranslate"><span class="pre">WindowPartition</span></code>, <code class="docutils literal notranslate"><span class="pre">WindowDepartition</span></code> implementations<a class="headerlink" href="#swapaxes-windowpartition-windowdepartition-implementations" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://github.com/pytorch/vision/blob/945bdad7523806b15d3740ce6ace2fced9ef9d3b/torchvision/models/maxvit.py#L213">PyTorch source implementation</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">SwapAxes</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">a</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">a</span> <span class="o">=</span> <span class="n">a</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">b</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">:</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">a</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">res</span>


<span class="k">class</span><span class="w"> </span><span class="nc">WindowPartition</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span> <span class="n">p</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">:</span>
        <span class="c1"># Output array with expected layout of [B, H/P, W/P, P*P, C].</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">P</span> <span class="o">=</span> <span class="n">p</span>
        <span class="c1"># chunk up H and W dimensions</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span> <span class="o">//</span> <span class="n">P</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">W</span> <span class="o">//</span> <span class="n">P</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">C</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">permute_dims</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
        <span class="c1"># colapse P * P dimension</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">B</span><span class="p">,</span> <span class="p">(</span><span class="n">H</span> <span class="o">//</span> <span class="n">P</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">W</span> <span class="o">//</span> <span class="n">P</span><span class="p">),</span> <span class="n">P</span> <span class="o">*</span> <span class="n">P</span><span class="p">,</span> <span class="n">C</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x</span>


<span class="k">class</span><span class="w"> </span><span class="nc">WindowDepartition</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span> <span class="n">p</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">h_partitions</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">w_partitions</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">:</span>
        <span class="c1"># Output array with expected layout of [B, H, W, C].</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">G</span><span class="p">,</span> <span class="n">PP</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">P</span> <span class="o">=</span> <span class="n">p</span>
        <span class="n">HP</span><span class="p">,</span> <span class="n">WP</span> <span class="o">=</span> <span class="n">h_partitions</span><span class="p">,</span> <span class="n">w_partitions</span>
        <span class="c1"># split P * P dimension into 2 P tile dimensions</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">B</span><span class="p">,</span> <span class="n">HP</span><span class="p">,</span> <span class="n">WP</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">C</span><span class="p">))</span>
        <span class="c1"># permute into B, HP, P, WP, P, C</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">permute_dims</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
        <span class="c1"># reshape into B, H, W, C</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">B</span><span class="p">,</span> <span class="n">HP</span> <span class="o">*</span> <span class="n">P</span><span class="p">,</span> <span class="n">WP</span> <span class="o">*</span> <span class="n">P</span><span class="p">,</span> <span class="n">C</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">mod</span> <span class="o">=</span> <span class="n">SwapAxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">mod</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">mod</span> <span class="o">=</span> <span class="n">WindowPartition</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">mod</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="p">(</span><span class="mi">128</span> <span class="o">//</span> <span class="mi">16</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">128</span> <span class="o">//</span> <span class="mi">16</span><span class="p">),</span> <span class="mi">16</span> <span class="o">*</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="p">(</span><span class="mi">128</span> <span class="o">//</span> <span class="mi">16</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">128</span> <span class="o">//</span> <span class="mi">16</span><span class="p">),</span> <span class="mi">16</span> <span class="o">*</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">mod</span> <span class="o">=</span> <span class="n">WindowDepartition</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">mod</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">h_partitions</span><span class="o">=</span><span class="mi">128</span> <span class="o">//</span> <span class="mi">16</span><span class="p">,</span> <span class="n">w_partitions</span><span class="o">=</span><span class="mi">128</span> <span class="o">//</span> <span class="mi">16</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="partitionattentionlayer-implementation">
<h3><code class="docutils literal notranslate"><span class="pre">PartitionAttentionLayer</span></code> implementation<a class="headerlink" href="#partitionattentionlayer-implementation" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://github.com/pytorch/vision/blob/945bdad7523806b15d3740ce6ace2fced9ef9d3b/torchvision/models/maxvit.py#L282">PyTorch source implementation</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">PartitionAttentionLayer</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Layer for partitioning the input tensor into non-overlapping windows and</span>
<span class="sd">    applying attention to each window.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">head_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="c1"># partitioning parameters</span>
        <span class="n">partition_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">partition_type</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="c1"># grid size needs to be known at initialization time</span>
        <span class="c1"># because we need to know hamy relative offsets there are in the grid</span>
        <span class="n">grid_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
        <span class="n">mlp_ratio</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">norm_layer</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">],</span>
        <span class="n">attention_dropout</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">mlp_dropout</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">p_stochastic_dropout</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">rngs</span><span class="p">:</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">in_channels</span> <span class="o">//</span> <span class="n">head_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">head_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_partitions</span> <span class="o">=</span> <span class="n">grid_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="n">partition_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">partition_type</span> <span class="o">=</span> <span class="n">partition_type</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grid_size</span> <span class="o">=</span> <span class="n">grid_size</span>

        <span class="k">if</span> <span class="n">partition_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;grid&quot;</span><span class="p">,</span> <span class="s2">&quot;window&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;partition_type must be either &#39;grid&#39; or &#39;window&#39;&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">partition_type</span> <span class="o">==</span> <span class="s2">&quot;window&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">p</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">g</span> <span class="o">=</span> <span class="n">partition_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_partitions</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">p</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">g</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_partitions</span><span class="p">,</span> <span class="n">partition_size</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">partition_op</span> <span class="o">=</span> <span class="n">WindowPartition</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">departition_op</span> <span class="o">=</span> <span class="n">WindowDepartition</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">partition_swap</span> <span class="o">=</span> <span class="n">SwapAxes</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">)</span> <span class="k">if</span> <span class="n">partition_type</span> <span class="o">==</span> <span class="s2">&quot;grid&quot;</span> <span class="k">else</span> <span class="n">Identity</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">departition_swap</span> <span class="o">=</span> <span class="n">SwapAxes</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">)</span> <span class="k">if</span> <span class="n">partition_type</span> <span class="o">==</span> <span class="s2">&quot;grid&quot;</span> <span class="k">else</span> <span class="n">Identity</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">attn_layer</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">norm_layer</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">),</span>
            <span class="c1"># it&#39;s always going to be partition_size ** 2 because</span>
            <span class="c1"># of the axis swap in the case of grid partitioning</span>
            <span class="n">RelativePositionalMultiHeadAttention</span><span class="p">(</span>
                <span class="n">in_channels</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">,</span> <span class="n">partition_size</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span>
            <span class="p">),</span>
            <span class="n">nnx</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">attention_dropout</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="c1"># pre-normalization similar to transformer layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_layer</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nnx</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">),</span>
            <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">in_channels</span> <span class="o">*</span> <span class="n">mlp_ratio</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">),</span>
            <span class="n">activation_layer</span><span class="p">,</span>
            <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_channels</span> <span class="o">*</span> <span class="n">mlp_ratio</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">),</span>
            <span class="n">nnx</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">mlp_dropout</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="c1"># layer scale factors</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stochastic_dropout</span> <span class="o">=</span> <span class="n">StochasticDepth</span><span class="p">(</span><span class="n">p_stochastic_dropout</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;row&quot;</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">:</span>
        <span class="c1"># Undefined behavior if H or W are not divisible by p</span>
        <span class="c1"># https://github.com/google-research/maxvit/blob/da76cf0d8a6ec668cc31b399c4126186da7da944/maxvit/models/maxvit.py#L766</span>
        <span class="n">gh</span><span class="p">,</span> <span class="n">gw</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grid_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">p</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">grid_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">p</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_assert</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">grid_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">grid_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span>
            <span class="s2">&quot;Grid size must be divisible by partition size. Got grid size of </span><span class="si">{}</span><span class="s2"> and partition size of </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">grid_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">p</span>
            <span class="p">),</span>
        <span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">partition_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">p</span><span class="p">)</span>  <span class="c1"># (B, H, W, C) -&gt; (B, H/P, W/P, P*P, C)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">partition_swap</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># -&gt; grid: (B, H/P, P*P, W/P, C)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">stochastic_dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">attn_layer</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">stochastic_dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mlp_layer</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">departition_swap</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># grid: (B, H/P, P*P, W/P, C) -&gt; (B, H/P, W/P, P*P, C)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">departition_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">p</span><span class="p">,</span> <span class="n">gh</span><span class="p">,</span> <span class="n">gw</span><span class="p">)</span>  <span class="c1"># -&gt; (B, H, W, C)</span>

        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">36</span><span class="p">))</span>

<span class="n">grid_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>
<span class="n">mod</span> <span class="o">=</span> <span class="n">PartitionAttentionLayer</span><span class="p">(</span>
    <span class="mi">36</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="s2">&quot;window&quot;</span><span class="p">,</span> <span class="n">grid_size</span><span class="o">=</span><span class="n">grid_size</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">activation_layer</span><span class="o">=</span><span class="n">nnx</span><span class="o">.</span><span class="n">gelu</span><span class="p">,</span> <span class="n">norm_layer</span><span class="o">=</span><span class="n">nnx</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">,</span>
    <span class="n">attention_dropout</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">mlp_dropout</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">p_stochastic_dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">mod</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">mod</span> <span class="o">=</span> <span class="n">PartitionAttentionLayer</span><span class="p">(</span>
    <span class="mi">36</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="s2">&quot;grid&quot;</span><span class="p">,</span> <span class="n">grid_size</span><span class="o">=</span><span class="n">grid_size</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">activation_layer</span><span class="o">=</span><span class="n">nnx</span><span class="o">.</span><span class="n">gelu</span><span class="p">,</span> <span class="n">norm_layer</span><span class="o">=</span><span class="n">nnx</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">,</span>
    <span class="n">attention_dropout</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">mlp_dropout</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">p_stochastic_dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">mod</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(4, 224, 224, 36)
(4, 224, 224, 36)
</pre></div>
</div>
</div>
</div>
</section>
<section id="maxvitlayer-implementation">
<h3><code class="docutils literal notranslate"><span class="pre">MaxVitLayer</span></code> implementation<a class="headerlink" href="#maxvitlayer-implementation" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://github.com/pytorch/vision/blob/945bdad7523806b15d3740ce6ace2fced9ef9d3b/torchvision/models/maxvit.py#L386">PyTorch source implementation</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">MaxVitLayer</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    MaxVit layer consisting of a MBConv layer followed by a PartitionAttentionLayer with `window`</span>
<span class="sd">    and a PartitionAttentionLayer with `grid`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="c1"># conv parameters</span>
        <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">squeeze_ratio</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">expansion_ratio</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="c1"># conv + transformer parameters</span>
        <span class="n">norm_layer</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">],</span>
        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="c1"># transformer parameters</span>
        <span class="n">head_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">mlp_ratio</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">mlp_dropout</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">attention_dropout</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">p_stochastic_dropout</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="c1"># partitioning parameters</span>
        <span class="n">partition_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">grid_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
        <span class="n">rngs</span><span class="p">:</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
    <span class="p">):</span>
        <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span>
            <span class="c1"># convolutional layer</span>
            <span class="n">MBConv</span><span class="p">(</span>
                <span class="n">in_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span>
                <span class="n">out_channels</span><span class="o">=</span><span class="n">out_channels</span><span class="p">,</span>
                <span class="n">expansion_ratio</span><span class="o">=</span><span class="n">expansion_ratio</span><span class="p">,</span>
                <span class="n">squeeze_ratio</span><span class="o">=</span><span class="n">squeeze_ratio</span><span class="p">,</span>
                <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
                <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
                <span class="n">norm_layer</span><span class="o">=</span><span class="n">norm_layer</span><span class="p">,</span>
                <span class="n">p_stochastic_dropout</span><span class="o">=</span><span class="n">p_stochastic_dropout</span><span class="p">,</span>
                <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="c1"># window_attention</span>
            <span class="n">PartitionAttentionLayer</span><span class="p">(</span>
                <span class="n">in_channels</span><span class="o">=</span><span class="n">out_channels</span><span class="p">,</span>
                <span class="n">head_dim</span><span class="o">=</span><span class="n">head_dim</span><span class="p">,</span>
                <span class="n">partition_size</span><span class="o">=</span><span class="n">partition_size</span><span class="p">,</span>
                <span class="n">partition_type</span><span class="o">=</span><span class="s2">&quot;window&quot;</span><span class="p">,</span>
                <span class="n">grid_size</span><span class="o">=</span><span class="n">grid_size</span><span class="p">,</span>
                <span class="n">mlp_ratio</span><span class="o">=</span><span class="n">mlp_ratio</span><span class="p">,</span>
                <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
                <span class="n">norm_layer</span><span class="o">=</span><span class="n">nnx</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">,</span>
                <span class="n">attention_dropout</span><span class="o">=</span><span class="n">attention_dropout</span><span class="p">,</span>
                <span class="n">mlp_dropout</span><span class="o">=</span><span class="n">mlp_dropout</span><span class="p">,</span>
                <span class="n">p_stochastic_dropout</span><span class="o">=</span><span class="n">p_stochastic_dropout</span><span class="p">,</span>
                <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="c1"># grid_attention</span>
            <span class="n">PartitionAttentionLayer</span><span class="p">(</span>
                <span class="n">in_channels</span><span class="o">=</span><span class="n">out_channels</span><span class="p">,</span>
                <span class="n">head_dim</span><span class="o">=</span><span class="n">head_dim</span><span class="p">,</span>
                <span class="n">partition_size</span><span class="o">=</span><span class="n">partition_size</span><span class="p">,</span>
                <span class="n">partition_type</span><span class="o">=</span><span class="s2">&quot;grid&quot;</span><span class="p">,</span>
                <span class="n">grid_size</span><span class="o">=</span><span class="n">grid_size</span><span class="p">,</span>
                <span class="n">mlp_ratio</span><span class="o">=</span><span class="n">mlp_ratio</span><span class="p">,</span>
                <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
                <span class="n">norm_layer</span><span class="o">=</span><span class="n">nnx</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">,</span>
                <span class="n">attention_dropout</span><span class="o">=</span><span class="n">attention_dropout</span><span class="p">,</span>
                <span class="n">mlp_dropout</span><span class="o">=</span><span class="n">mlp_dropout</span><span class="p">,</span>
                <span class="n">p_stochastic_dropout</span><span class="o">=</span><span class="n">p_stochastic_dropout</span><span class="p">,</span>
                <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_get_conv_output_shape</span><span class="p">(</span>
    <span class="n">input_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="n">kernel_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">padding</span><span class="p">:</span> <span class="nb">int</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="p">(</span><span class="n">input_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">kernel_size</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">padding</span><span class="p">)</span> <span class="o">//</span> <span class="n">stride</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
        <span class="p">(</span><span class="n">input_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">kernel_size</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">padding</span><span class="p">)</span> <span class="o">//</span> <span class="n">stride</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="n">grid_size</span> <span class="o">=</span> <span class="n">_get_conv_output_shape</span><span class="p">((</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">),</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">norm_layer</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.99</span><span class="p">)</span>

<span class="n">mod</span> <span class="o">=</span> <span class="n">MaxVitLayer</span><span class="p">(</span>
    <span class="mi">3</span><span class="p">,</span> <span class="mi">36</span><span class="p">,</span> <span class="n">squeeze_ratio</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">expansion_ratio</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">norm_layer</span><span class="o">=</span><span class="n">norm_layer</span><span class="p">,</span> <span class="n">activation_layer</span><span class="o">=</span><span class="n">nnx</span><span class="o">.</span><span class="n">gelu</span><span class="p">,</span>
    <span class="n">head_dim</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">mlp_dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
    <span class="n">attention_dropout</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">p_stochastic_dropout</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
    <span class="n">partition_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">grid_size</span><span class="o">=</span><span class="n">grid_size</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">mod</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(4, 112, 112, 36)
</pre></div>
</div>
</div>
</div>
</section>
<section id="maxvitblock-implementation">
<h3><code class="docutils literal notranslate"><span class="pre">MaxVitBlock</span></code> implementation<a class="headerlink" href="#maxvitblock-implementation" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://github.com/pytorch/vision/blob/945bdad7523806b15d3740ce6ace2fced9ef9d3b/torchvision/models/maxvit.py#L483">PyTorch source implementation</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">MaxVitBlock</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A MaxVit block consisting of `n_layers` MaxVit layers.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="c1"># conv parameters</span>
        <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">squeeze_ratio</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">expansion_ratio</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="c1"># conv + transformer parameters</span>
        <span class="n">norm_layer</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">],</span>
        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="c1"># transformer parameters</span>
        <span class="n">head_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">mlp_ratio</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">mlp_dropout</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">attention_dropout</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="c1"># partitioning parameters</span>
        <span class="n">partition_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">input_grid_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
        <span class="c1"># number of layers</span>
        <span class="n">n_layers</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">p_stochastic</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
        <span class="n">rngs</span><span class="p">:</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">len</span><span class="p">(</span><span class="n">p_stochastic</span><span class="p">)</span> <span class="o">==</span> <span class="n">n_layers</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;p_stochastic must have length n_layers=</span><span class="si">{</span><span class="n">n_layers</span><span class="si">}</span><span class="s2">, got p_stochastic=</span><span class="si">{</span><span class="n">p_stochastic</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

        <span class="c1"># account for the first stride of the first layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grid_size</span> <span class="o">=</span> <span class="n">_get_conv_output_shape</span><span class="p">(</span><span class="n">input_grid_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">p_stochastic</span><span class="p">):</span>
            <span class="n">stride</span> <span class="o">=</span> <span class="mi">2</span> <span class="k">if</span> <span class="n">idx</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">1</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">MaxVitLayer</span><span class="p">(</span>
                    <span class="n">in_channels</span><span class="o">=</span><span class="n">in_channels</span> <span class="k">if</span> <span class="n">idx</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">out_channels</span><span class="p">,</span>
                    <span class="n">out_channels</span><span class="o">=</span><span class="n">out_channels</span><span class="p">,</span>
                    <span class="n">squeeze_ratio</span><span class="o">=</span><span class="n">squeeze_ratio</span><span class="p">,</span>
                    <span class="n">expansion_ratio</span><span class="o">=</span><span class="n">expansion_ratio</span><span class="p">,</span>
                    <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
                    <span class="n">norm_layer</span><span class="o">=</span><span class="n">norm_layer</span><span class="p">,</span>
                    <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
                    <span class="n">head_dim</span><span class="o">=</span><span class="n">head_dim</span><span class="p">,</span>
                    <span class="n">mlp_ratio</span><span class="o">=</span><span class="n">mlp_ratio</span><span class="p">,</span>
                    <span class="n">mlp_dropout</span><span class="o">=</span><span class="n">mlp_dropout</span><span class="p">,</span>
                    <span class="n">attention_dropout</span><span class="o">=</span><span class="n">attention_dropout</span><span class="p">,</span>
                    <span class="n">partition_size</span><span class="o">=</span><span class="n">partition_size</span><span class="p">,</span>
                    <span class="n">grid_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">grid_size</span><span class="p">,</span>
                    <span class="n">p_stochastic_dropout</span><span class="o">=</span><span class="n">p</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="n">input_grid_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>
<span class="n">norm_layer</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.99</span><span class="p">)</span>

<span class="n">mod</span> <span class="o">=</span> <span class="n">MaxVitBlock</span><span class="p">(</span>
    <span class="mi">3</span><span class="p">,</span> <span class="mi">36</span><span class="p">,</span> <span class="n">squeeze_ratio</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">expansion_ratio</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">norm_layer</span><span class="o">=</span><span class="n">norm_layer</span><span class="p">,</span> <span class="n">activation_layer</span><span class="o">=</span><span class="n">nnx</span><span class="o">.</span><span class="n">gelu</span><span class="p">,</span>
    <span class="n">head_dim</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">mlp_dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">attention_dropout</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span>
    <span class="n">partition_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">input_grid_size</span><span class="o">=</span><span class="n">input_grid_size</span><span class="p">,</span>
    <span class="n">n_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">p_stochastic</span><span class="o">=</span><span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span>
<span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">mod</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(4, 112, 112, 36)
</pre></div>
</div>
</div>
</div>
</section>
<section id="maxvit-implementation">
<h3><code class="docutils literal notranslate"><span class="pre">MaxVit</span></code> implementation<a class="headerlink" href="#maxvit-implementation" title="Link to this heading">#</a></h3>
<p>Finally, we can assemble everything together and define the MaxVit model.
<a class="reference external" href="https://github.com/pytorch/vision/blob/945bdad7523806b15d3740ce6ace2fced9ef9d3b/torchvision/models/maxvit.py#L568">PyTorch source implementation</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_make_block_input_shapes</span><span class="p">(</span><span class="n">input_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="n">n_blocks</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Util function to check that the input size is correct for a MaxVit configuration.&quot;&quot;&quot;</span>
    <span class="n">shapes</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">block_input_shape</span> <span class="o">=</span> <span class="n">_get_conv_output_shape</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_blocks</span><span class="p">):</span>
        <span class="n">block_input_shape</span> <span class="o">=</span> <span class="n">_get_conv_output_shape</span><span class="p">(</span><span class="n">block_input_shape</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">shapes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">block_input_shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">shapes</span>


<span class="k">class</span><span class="w"> </span><span class="nc">MaxVit</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implements MaxVit Transformer from the &quot;MaxViT: Multi-Axis Vision Transformer&quot; paper.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="c1"># input size parameters</span>
        <span class="n">input_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
        <span class="c1"># stem and task parameters</span>
        <span class="n">stem_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="c1"># partitioning parameters</span>
        <span class="n">partition_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="c1"># block parameters</span>
        <span class="n">block_channels</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">block_layers</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="c1"># attention head dimensions</span>
        <span class="n">head_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">stochastic_depth_prob</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="c1"># conv + transformer parameters</span>
        <span class="c1"># norm_layer is applied only to the conv layers</span>
        <span class="c1"># activation_layer is applied both to conv and transformer layers</span>
        <span class="n">norm_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">activation_layer</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">gelu</span><span class="p">,</span>
        <span class="c1"># conv parameters</span>
        <span class="n">squeeze_ratio</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">,</span>
        <span class="n">expansion_ratio</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
        <span class="c1"># transformer parameters</span>
        <span class="n">mlp_ratio</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
        <span class="n">mlp_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="n">attention_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="c1"># task parameters</span>
        <span class="n">num_classes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
        <span class="n">rngs</span><span class="p">:</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
    <span class="p">):</span>
        <span class="n">input_channels</span> <span class="o">=</span> <span class="mi">3</span>

        <span class="k">if</span> <span class="n">norm_layer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">norm_layer</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.99</span><span class="p">)</span>

        <span class="c1"># Make sure input size will be divisible by the partition size in all blocks</span>
        <span class="c1"># Undefined behavior if H or W are not divisible by p</span>
        <span class="n">block_input_sizes</span> <span class="o">=</span> <span class="n">_make_block_input_shapes</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">block_channels</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">block_input_size</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">block_input_sizes</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">block_input_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">%</span> <span class="n">partition_size</span> <span class="o">!=</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">block_input_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">%</span> <span class="n">partition_size</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Input size </span><span class="si">{</span><span class="n">block_input_size</span><span class="si">}</span><span class="s2"> of block </span><span class="si">{</span><span class="n">idx</span><span class="si">}</span><span class="s2"> is not divisible by partition size </span><span class="si">{</span><span class="n">partition_size</span><span class="si">}</span><span class="s2">. &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;Consider changing the partition size or the input size.</span><span class="se">\n</span><span class="s2">&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;Current configuration yields the following block input sizes: </span><span class="si">{</span><span class="n">block_input_sizes</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="p">)</span>

        <span class="c1"># stem</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stem</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">Conv2dNormActivation</span><span class="p">(</span>
                <span class="n">input_channels</span><span class="p">,</span>
                <span class="n">stem_channels</span><span class="p">,</span>
                <span class="mi">3</span><span class="p">,</span>
                <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                <span class="n">norm_layer</span><span class="o">=</span><span class="n">norm_layer</span><span class="p">,</span>
                <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
                <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="n">Conv2dNormActivation</span><span class="p">(</span>
                <span class="n">stem_channels</span><span class="p">,</span>
                <span class="n">stem_channels</span><span class="p">,</span>
                <span class="mi">3</span><span class="p">,</span>
                <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">norm_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">activation_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>

        <span class="c1"># account for stem stride</span>
        <span class="n">input_size</span> <span class="o">=</span> <span class="n">_get_conv_output_shape</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">partition_size</span> <span class="o">=</span> <span class="n">partition_size</span>

        <span class="c1"># blocks</span>
        <span class="n">blocks</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">in_channels</span> <span class="o">=</span> <span class="p">[</span><span class="n">stem_channels</span><span class="p">]</span> <span class="o">+</span> <span class="n">block_channels</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">out_channels</span> <span class="o">=</span> <span class="n">block_channels</span>

        <span class="c1"># precompute the stochastic depth probabilities from 0 to stochastic_depth_prob</span>
        <span class="c1"># since we have N blocks with L layers, we will have N * L probabilities uniformly distributed</span>
        <span class="c1"># over the range [0, stochastic_depth_prob]</span>
        <span class="n">p_stochastic</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">stochastic_depth_prob</span><span class="p">,</span> <span class="nb">sum</span><span class="p">(</span><span class="n">block_layers</span><span class="p">))</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

        <span class="n">p_idx</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">in_channel</span><span class="p">,</span> <span class="n">out_channel</span><span class="p">,</span> <span class="n">num_layers</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">block_layers</span><span class="p">):</span>
            <span class="n">blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">MaxVitBlock</span><span class="p">(</span>
                    <span class="n">in_channels</span><span class="o">=</span><span class="n">in_channel</span><span class="p">,</span>
                    <span class="n">out_channels</span><span class="o">=</span><span class="n">out_channel</span><span class="p">,</span>
                    <span class="n">squeeze_ratio</span><span class="o">=</span><span class="n">squeeze_ratio</span><span class="p">,</span>
                    <span class="n">expansion_ratio</span><span class="o">=</span><span class="n">expansion_ratio</span><span class="p">,</span>
                    <span class="n">norm_layer</span><span class="o">=</span><span class="n">norm_layer</span><span class="p">,</span>
                    <span class="n">activation_layer</span><span class="o">=</span><span class="n">activation_layer</span><span class="p">,</span>
                    <span class="n">head_dim</span><span class="o">=</span><span class="n">head_dim</span><span class="p">,</span>
                    <span class="n">mlp_ratio</span><span class="o">=</span><span class="n">mlp_ratio</span><span class="p">,</span>
                    <span class="n">mlp_dropout</span><span class="o">=</span><span class="n">mlp_dropout</span><span class="p">,</span>
                    <span class="n">attention_dropout</span><span class="o">=</span><span class="n">attention_dropout</span><span class="p">,</span>
                    <span class="n">partition_size</span><span class="o">=</span><span class="n">partition_size</span><span class="p">,</span>
                    <span class="n">input_grid_size</span><span class="o">=</span><span class="n">input_size</span><span class="p">,</span>
                    <span class="n">n_layers</span><span class="o">=</span><span class="n">num_layers</span><span class="p">,</span>
                    <span class="n">p_stochastic</span><span class="o">=</span><span class="n">p_stochastic</span><span class="p">[</span><span class="n">p_idx</span> <span class="p">:</span> <span class="n">p_idx</span> <span class="o">+</span> <span class="n">num_layers</span><span class="p">],</span>
                <span class="p">),</span>
            <span class="p">)</span>
            <span class="n">input_size</span> <span class="o">=</span> <span class="n">blocks</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">grid_size</span>
            <span class="n">p_idx</span> <span class="o">+=</span> <span class="n">num_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">blocks</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">nnx</span><span class="o">.</span><span class="n">avg_pool</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])),</span>  <span class="c1">#  nn.AdaptiveAvgPool2d(1)</span>
            <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span>  <span class="c1">#  nn.Flatten()</span>
            <span class="n">nnx</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">block_channels</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">),</span>
            <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">block_channels</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">block_channels</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">),</span>
            <span class="n">nnx</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span>
            <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">block_channels</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">num_classes</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_init_weights</span><span class="p">(</span><span class="n">rngs</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_init_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rngs</span><span class="p">):</span>
        <span class="n">normal_initializer</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">stddev</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">iter_modules</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Conv</span><span class="p">,</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">)):</span>
                <span class="n">module</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">normal_initializer</span><span class="p">(</span>
                    <span class="n">rngs</span><span class="p">(),</span> <span class="n">module</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">module</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">dtype</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="n">module</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">module</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                        <span class="n">module</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">module</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">dtype</span>
                    <span class="p">)</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nnx</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">):</span>
                <span class="n">module</span><span class="o">.</span><span class="n">scale</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">scale</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">module</span><span class="o">.</span><span class="n">scale</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
                <span class="n">module</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">module</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="n">mod</span> <span class="o">=</span> <span class="n">MaxVit</span><span class="p">(</span>
    <span class="n">input_size</span><span class="o">=</span><span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">),</span>
    <span class="n">stem_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="n">block_channels</span><span class="o">=</span><span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">],</span>
    <span class="n">block_layers</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
    <span class="n">head_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">stochastic_depth_prob</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">partition_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
    <span class="n">num_classes</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">mod</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(4, 1000)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">maxvit_t</span><span class="p">(</span>
    <span class="n">input_size</span><span class="o">=</span><span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">),</span>
    <span class="n">stem_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="n">block_channels</span><span class="o">=</span><span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">],</span>
    <span class="n">block_layers</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
    <span class="n">head_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">stochastic_depth_prob</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">partition_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
    <span class="n">num_classes</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
<span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">MaxVit</span><span class="p">(</span>
        <span class="n">input_size</span><span class="o">=</span><span class="n">input_size</span><span class="p">,</span>
        <span class="n">stem_channels</span><span class="o">=</span><span class="n">stem_channels</span><span class="p">,</span>
        <span class="n">block_channels</span><span class="o">=</span><span class="n">block_channels</span><span class="p">,</span>
        <span class="n">block_layers</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
        <span class="n">head_dim</span><span class="o">=</span><span class="n">head_dim</span><span class="p">,</span>
        <span class="n">stochastic_depth_prob</span><span class="o">=</span><span class="n">stochastic_depth_prob</span><span class="p">,</span>
        <span class="n">partition_size</span><span class="o">=</span><span class="n">partition_size</span><span class="p">,</span>
        <span class="n">num_classes</span><span class="o">=</span><span class="n">num_classes</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="test-jax-implementation">
<h3>Test JAX implementation<a class="headerlink" href="#test-jax-implementation" title="Link to this heading">#</a></h3>
<p>Let us import equivalent PyTorch modules and check our implementations against PyTorch. Please note that
PyTorch modules will contain random parameters and buffers that we need to set into our Flax implementations.</p>
<p>Below we define a helper class <code class="docutils literal notranslate"><span class="pre">Torch2Flax</span></code> to copy parameters and buffers from a PyTorch module into equivalent Flax module.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>


<span class="k">class</span><span class="w"> </span><span class="nc">Torch2Flax</span><span class="p">:</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">conv_params_permute</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">torch_param</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">name</span> <span class="o">==</span> <span class="s2">&quot;weight&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch_param</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch_param</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">linear_params_permute</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">torch_param</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">name</span> <span class="o">==</span> <span class="s2">&quot;weight&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch_param</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch_param</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">default_params_transform</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">param</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">param</span>

    <span class="n">modules_mapping_info</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Conv</span><span class="p">,</span>
            <span class="s2">&quot;params_mapping&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;weight&quot;</span><span class="p">:</span> <span class="s2">&quot;kernel&quot;</span><span class="p">,</span>
                <span class="s2">&quot;bias&quot;</span><span class="p">:</span> <span class="s2">&quot;bias&quot;</span><span class="p">,</span>
            <span class="p">},</span>
            <span class="s2">&quot;params_transform&quot;</span><span class="p">:</span> <span class="n">conv_params_permute</span><span class="p">,</span>
        <span class="p">},</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="n">nnx</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">,</span>
            <span class="s2">&quot;params_mapping&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;weight&quot;</span><span class="p">:</span> <span class="s2">&quot;scale&quot;</span><span class="p">,</span>
                <span class="s2">&quot;bias&quot;</span><span class="p">:</span> <span class="s2">&quot;bias&quot;</span><span class="p">,</span>
                <span class="s2">&quot;running_mean&quot;</span><span class="p">:</span> <span class="s2">&quot;mean&quot;</span><span class="p">,</span>
                <span class="s2">&quot;running_var&quot;</span><span class="p">:</span> <span class="s2">&quot;var&quot;</span><span class="p">,</span>
            <span class="p">},</span>
        <span class="p">},</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">,</span>
            <span class="s2">&quot;params_mapping&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;weight&quot;</span><span class="p">:</span> <span class="s2">&quot;kernel&quot;</span><span class="p">,</span>
                <span class="s2">&quot;bias&quot;</span><span class="p">:</span> <span class="s2">&quot;bias&quot;</span><span class="p">,</span>
            <span class="p">},</span>
            <span class="s2">&quot;params_transform&quot;</span><span class="p">:</span> <span class="n">linear_params_permute</span><span class="p">,</span>
        <span class="p">},</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="n">nnx</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">,</span>
            <span class="s2">&quot;params_mapping&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;weight&quot;</span><span class="p">:</span> <span class="s2">&quot;scale&quot;</span><span class="p">,</span>
                <span class="s2">&quot;bias&quot;</span><span class="p">:</span> <span class="s2">&quot;bias&quot;</span><span class="p">,</span>
            <span class="p">},</span>
        <span class="p">}</span>
    <span class="p">}</span> <span class="o">|</span> <span class="p">{</span>
        <span class="n">torch_mod</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="n">nnx_fn_type</span><span class="p">,</span>
            <span class="s2">&quot;params_mapping&quot;</span><span class="p">:</span> <span class="p">{},</span>
        <span class="p">}</span> <span class="k">for</span> <span class="n">torch_mod</span><span class="p">,</span> <span class="n">nnx_fn_type</span> <span class="ow">in</span> <span class="p">[</span>
            <span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">,</span> <span class="n">Identity</span><span class="p">),</span>
            <span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">)),</span>
            <span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">relu</span><span class="p">)),</span>
            <span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">gelu</span><span class="p">)),</span>
            <span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">SELU</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">selu</span><span class="p">)),</span>
            <span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">SiLU</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">silu</span><span class="p">)),</span>
            <span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">tanh</span><span class="p">)),</span>
            <span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">,</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Dropout</span><span class="p">),</span>
            <span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">)),</span>
            <span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">nnx</span><span class="o">.</span><span class="n">avg_pool</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))),</span>
            <span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">nnx</span><span class="o">.</span><span class="n">avg_pool</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])))),</span>
        <span class="p">]</span>
    <span class="p">}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_copy_params_buffers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">torch_nn_module</span><span class="p">,</span> <span class="n">nnx_module</span><span class="p">):</span>
        <span class="n">torch_module_type</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">torch_nn_module</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">torch_module_type</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules_mapping_info</span><span class="p">,</span> <span class="n">torch_module_type</span>
        <span class="n">module_mapping_info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules_mapping_info</span><span class="p">[</span><span class="n">torch_module_type</span><span class="p">]</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">nnx_module</span><span class="p">,</span> <span class="n">module_mapping_info</span><span class="p">[</span><span class="s2">&quot;type&quot;</span><span class="p">]),</span> <span class="p">(</span>
            <span class="n">nnx_module</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">nnx_module</span><span class="p">),</span> <span class="n">module_mapping_info</span><span class="p">[</span><span class="s2">&quot;type&quot;</span><span class="p">]</span>
        <span class="p">)</span>

        <span class="k">for</span> <span class="n">torch_key</span><span class="p">,</span> <span class="n">nnx_key</span> <span class="ow">in</span> <span class="n">module_mapping_info</span><span class="p">[</span><span class="s2">&quot;params_mapping&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>

            <span class="n">torch_value</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">torch_nn_module</span><span class="p">,</span> <span class="n">torch_key</span><span class="p">)</span>
            <span class="n">nnx_param</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">nnx_module</span><span class="p">,</span> <span class="n">nnx_key</span><span class="p">)</span>
            <span class="k">assert</span> <span class="n">nnx_param</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="p">(</span><span class="n">torch_key</span><span class="p">,</span> <span class="n">nnx_key</span><span class="p">,</span> <span class="n">nnx_module</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">torch_value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">assert</span> <span class="n">nnx_param</span><span class="o">.</span><span class="n">value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">,</span> <span class="n">nnx_param</span>
                <span class="k">continue</span>

            <span class="n">params_transform</span> <span class="o">=</span> <span class="n">module_mapping_info</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;params_transform&quot;</span><span class="p">,</span> <span class="n">Torch2Flax</span><span class="o">.</span><span class="n">default_params_transform</span><span class="p">)</span>
            <span class="n">torch_value</span> <span class="o">=</span> <span class="n">params_transform</span><span class="p">(</span><span class="n">torch_key</span><span class="p">,</span> <span class="n">torch_value</span><span class="p">)</span>

            <span class="k">assert</span> <span class="n">nnx_param</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">torch_value</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="p">(</span>
                <span class="n">nnx_key</span><span class="p">,</span> <span class="n">nnx_param</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">torch_key</span><span class="p">,</span> <span class="n">torch_value</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span>
            <span class="p">)</span>
            <span class="n">nnx_param</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">torch_value</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_copy_sequential</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">torch_nn_seq</span><span class="p">,</span> <span class="n">nnx_seq</span><span class="p">,</span> <span class="n">skip_modules</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">torch_nn_seq</span><span class="p">,</span> <span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">)),</span> <span class="nb">type</span><span class="p">(</span><span class="n">torch_nn_seq</span><span class="p">)</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">nnx_seq</span><span class="p">,</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Sequential</span><span class="p">),</span> <span class="nb">type</span><span class="p">(</span><span class="n">nnx_seq</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">torch_nn_seq</span><span class="p">):</span>
            <span class="n">torch_module</span> <span class="o">=</span> <span class="n">torch_nn_seq</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">nnx_module</span> <span class="o">=</span> <span class="n">nnx_seq</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">copy_module</span><span class="p">(</span><span class="n">torch_module</span><span class="p">,</span> <span class="n">nnx_module</span><span class="p">,</span> <span class="n">skip_modules</span><span class="o">=</span><span class="n">skip_modules</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">copy_module</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">torch_module</span><span class="p">,</span> <span class="n">nnx_module</span><span class="p">,</span> <span class="n">skip_modules</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">skip_modules</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">skip_modules</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">torch_module</span><span class="p">,</span> <span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">)):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_copy_sequential</span><span class="p">(</span><span class="n">torch_module</span><span class="p">,</span> <span class="n">nnx_module</span><span class="p">,</span> <span class="n">skip_modules</span><span class="o">=</span><span class="n">skip_modules</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">torch_module</span><span class="p">)</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules_mapping_info</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_copy_params_buffers</span><span class="p">(</span><span class="n">torch_module</span><span class="p">,</span> <span class="n">nnx_module</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">skip_modules</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">torch_module</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="ow">in</span> <span class="n">skip_modules</span><span class="p">:</span>
                    <span class="k">return</span>

            <span class="n">named_children</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">torch_module</span><span class="o">.</span><span class="n">named_children</span><span class="p">())</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">named_children</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">torch_module</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">torch_child</span> <span class="ow">in</span> <span class="n">named_children</span><span class="p">:</span>
                <span class="n">nnx_child</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">nnx_module</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
                <span class="k">assert</span> <span class="n">nnx_child</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">nnx_module</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">copy_module</span><span class="p">(</span><span class="n">torch_child</span><span class="p">,</span> <span class="n">nnx_child</span><span class="p">,</span> <span class="n">skip_modules</span><span class="o">=</span><span class="n">skip_modules</span><span class="p">)</span>
            <span class="c1"># Copy buffers and params of the module itself (not its children)</span>
            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">torch_buffer</span> <span class="ow">in</span> <span class="n">torch_module</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">():</span>
                <span class="k">if</span> <span class="s2">&quot;.&quot;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
                    <span class="c1"># This is child&#39;s buffer</span>
                    <span class="k">continue</span>
                <span class="n">nnx_buffer</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">nnx_module</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">nnx_buffer</span><span class="p">,</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Variable</span><span class="p">),</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">nnx_buffer</span><span class="p">,</span> <span class="n">nnx_module</span><span class="p">)</span>

                <span class="k">assert</span> <span class="n">nnx_buffer</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">torch_buffer</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="p">(</span>
                    <span class="n">name</span><span class="p">,</span> <span class="n">nnx_buffer</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">torch_buffer</span><span class="o">.</span><span class="n">shape</span>
                <span class="p">)</span>
                <span class="n">nnx_buffer</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">torch_buffer</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">torch_param</span> <span class="ow">in</span> <span class="n">torch_module</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
                <span class="k">if</span> <span class="s2">&quot;.&quot;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
                    <span class="c1"># This is child&#39;s parameter</span>
                    <span class="k">continue</span>
                <span class="n">nnx_param</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">nnx_module</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">nnx_param</span><span class="p">,</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Param</span><span class="p">),</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">nnx_param</span><span class="p">,</span> <span class="n">nnx_module</span><span class="p">)</span>

                <span class="k">assert</span> <span class="n">nnx_param</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">torch_param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="p">(</span>
                    <span class="n">name</span><span class="p">,</span> <span class="n">nnx_param</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">torch_param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span>
                <span class="p">)</span>
                <span class="n">nnx_param</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">torch_param</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">test_modules</span><span class="p">(</span>
    <span class="n">nnx_module</span><span class="p">,</span> <span class="n">torch_module</span><span class="p">,</span> <span class="n">torch_input</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;eval&quot;</span><span class="p">,</span> <span class="n">permute_torch_input</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span>
<span class="p">):</span>
    <span class="k">assert</span> <span class="n">torch_input</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">4</span>
    <span class="k">assert</span> <span class="n">mode</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;eval&quot;</span><span class="p">,</span> <span class="s2">&quot;train&quot;</span><span class="p">)</span>

    <span class="n">torch_input</span> <span class="o">=</span> <span class="n">torch_input</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">torch_module</span> <span class="o">=</span> <span class="n">torch_module</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;eval&quot;</span><span class="p">:</span>
        <span class="n">torch_module</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="n">nnx_module</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">torch_module</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="n">nnx_module</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="o">==</span><span class="s2">&quot;eval&quot;</span><span class="p">):</span>
        <span class="n">torch_output</span> <span class="o">=</span> <span class="n">torch_module</span><span class="p">(</span><span class="n">torch_input</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">permute_torch_input</span><span class="p">:</span>
        <span class="n">torch_input</span> <span class="o">=</span> <span class="n">torch_input</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">jax_input</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">torch_input</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">jax</span><span class="o">.</span><span class="n">devices</span><span class="p">(</span><span class="n">device</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">jax_output</span> <span class="o">=</span> <span class="n">nnx_module</span><span class="p">(</span><span class="n">jax_input</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">jax_output</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="n">jax</span><span class="o">.</span><span class="n">devices</span><span class="p">(</span><span class="n">device</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">torch_output</span> <span class="o">=</span> <span class="n">torch_output</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">permute_torch_input</span> <span class="ow">and</span> <span class="n">torch_output</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
        <span class="n">torch_output</span> <span class="o">=</span> <span class="n">torch_output</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">jax_expected</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">torch_output</span><span class="p">)</span>

    <span class="k">assert</span> <span class="n">jnp</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">jax_output</span><span class="p">,</span> <span class="n">jax_expected</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="n">atol</span><span class="p">),</span> <span class="p">(</span>
        <span class="n">jnp</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">jax_output</span> <span class="o">-</span> <span class="n">jax_expected</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span>
        <span class="n">jnp</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">jax_output</span> <span class="o">-</span> <span class="n">jax_expected</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span>
    <span class="p">)</span>


<span class="n">t2f</span> <span class="o">=</span> <span class="n">Torch2Flax</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Let us now test our JAX modules. We only test the result of the forward pass in the inference mode such that we avoid discrepancies related to random layers like <code class="docutils literal notranslate"><span class="pre">Dropout</span></code>, <code class="docutils literal notranslate"><span class="pre">StochasticDepth</span></code>, etc.
By default, we use absolute error tolerence <code class="docutils literal notranslate"><span class="pre">1e-3</span></code> when comparing the JAX output against expected PyTorch result.
For larger modules we set the device to CPU for the JAX model to execute on in order to reduce the errors between CPU and CUDA.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torchvision.ops.misc</span><span class="w"> </span><span class="kn">import</span> <span class="n">Conv2dNormActivation</span> <span class="k">as</span> <span class="n">PyTorchConv2dNormActivation</span>


<span class="n">torch_module</span> <span class="o">=</span> <span class="n">PyTorchConv2dNormActivation</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">nnx_module</span> <span class="o">=</span> <span class="n">Conv2dNormActivation</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">t2f</span><span class="o">.</span><span class="n">copy_module</span><span class="p">(</span><span class="n">torch_module</span><span class="p">,</span> <span class="n">nnx_module</span><span class="p">)</span>

<span class="n">test_modules</span><span class="p">(</span><span class="n">nnx_module</span><span class="p">,</span> <span class="n">torch_module</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">46</span><span class="p">,</span> <span class="mi">46</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torchvision.ops.misc</span><span class="w"> </span><span class="kn">import</span> <span class="n">SqueezeExcitation</span> <span class="k">as</span> <span class="n">PyTorchSqueezeExcitation</span>


<span class="n">torch_module</span> <span class="o">=</span> <span class="n">PyTorchSqueezeExcitation</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">nnx_module</span> <span class="o">=</span> <span class="n">SqueezeExcitation</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

<span class="n">t2f</span><span class="o">.</span><span class="n">copy_module</span><span class="p">(</span><span class="n">torch_module</span><span class="p">,</span> <span class="n">nnx_module</span><span class="p">)</span>

<span class="n">test_modules</span><span class="p">(</span><span class="n">nnx_module</span><span class="p">,</span> <span class="n">torch_module</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">46</span><span class="p">,</span> <span class="mi">46</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">functools</span><span class="w"> </span><span class="kn">import</span> <span class="n">partial</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchvision.models.maxvit</span><span class="w"> </span><span class="kn">import</span> <span class="n">MBConv</span> <span class="k">as</span> <span class="n">PyTorchMBConv</span>


<span class="n">norm_layer</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">torch_module</span> <span class="o">=</span> <span class="n">PyTorchMBConv</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">activation_layer</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">,</span> <span class="n">norm_layer</span><span class="o">=</span><span class="n">norm_layer</span><span class="p">)</span>

<span class="n">norm_layer</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.99</span><span class="p">)</span>
<span class="n">nnx_module</span> <span class="o">=</span> <span class="n">MBConv</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">activation_layer</span><span class="o">=</span><span class="n">nnx</span><span class="o">.</span><span class="n">gelu</span><span class="p">,</span> <span class="n">norm_layer</span><span class="o">=</span><span class="n">norm_layer</span><span class="p">)</span>


<span class="n">t2f</span><span class="o">.</span><span class="n">copy_module</span><span class="p">(</span><span class="n">torch_module</span><span class="p">,</span> <span class="n">nnx_module</span><span class="p">)</span>

<span class="n">test_modules</span><span class="p">(</span><span class="n">nnx_module</span><span class="p">,</span> <span class="n">torch_module</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">46</span><span class="p">,</span> <span class="mi">46</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torchvision.models.maxvit</span><span class="w"> </span><span class="kn">import</span> <span class="n">RelativePositionalMultiHeadAttention</span> <span class="k">as</span> <span class="n">PyTorchRelativePositionalMultiHeadAttention</span>


<span class="n">torch_module</span> <span class="o">=</span> <span class="n">PyTorchRelativePositionalMultiHeadAttention</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">49</span><span class="p">)</span>
<span class="n">nnx_module</span> <span class="o">=</span> <span class="n">RelativePositionalMultiHeadAttention</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">49</span><span class="p">)</span>

<span class="n">t2f</span><span class="o">.</span><span class="n">copy_module</span><span class="p">(</span><span class="n">torch_module</span><span class="p">,</span> <span class="n">nnx_module</span><span class="p">)</span>

<span class="n">test_modules</span><span class="p">(</span><span class="n">nnx_module</span><span class="p">,</span> <span class="n">torch_module</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">49</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span> <span class="n">permute_torch_input</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torchvision.models.maxvit</span><span class="w"> </span><span class="kn">import</span> <span class="n">PartitionAttentionLayer</span> <span class="k">as</span> <span class="n">PyTorchPartitionAttentionLayer</span>


<span class="n">grid_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>
<span class="k">for</span> <span class="n">partition_type</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;window&quot;</span><span class="p">,</span> <span class="s2">&quot;grid&quot;</span><span class="p">]:</span>

    <span class="n">torch_module</span> <span class="o">=</span> <span class="n">PyTorchPartitionAttentionLayer</span><span class="p">(</span>
        <span class="mi">36</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="n">partition_type</span><span class="p">,</span> <span class="n">grid_size</span><span class="o">=</span><span class="n">grid_size</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="n">activation_layer</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">,</span> <span class="n">norm_layer</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">,</span>
        <span class="n">attention_dropout</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">mlp_dropout</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">p_stochastic_dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">nnx_module</span> <span class="o">=</span> <span class="n">PartitionAttentionLayer</span><span class="p">(</span>
        <span class="mi">36</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="n">partition_type</span><span class="p">,</span> <span class="n">grid_size</span><span class="o">=</span><span class="n">grid_size</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="n">activation_layer</span><span class="o">=</span><span class="n">nnx</span><span class="o">.</span><span class="n">gelu</span><span class="p">,</span> <span class="n">norm_layer</span><span class="o">=</span><span class="n">nnx</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">,</span>
        <span class="n">attention_dropout</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">mlp_dropout</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">p_stochastic_dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">t2f</span><span class="o">.</span><span class="n">copy_module</span><span class="p">(</span><span class="n">torch_module</span><span class="p">,</span> <span class="n">nnx_module</span><span class="p">,</span> <span class="n">skip_modules</span><span class="o">=</span><span class="p">[</span>
        <span class="s2">&quot;WindowPartition&quot;</span><span class="p">,</span> <span class="s2">&quot;WindowDepartition&quot;</span><span class="p">,</span> <span class="s2">&quot;SwapAxes&quot;</span><span class="p">,</span> <span class="s2">&quot;StochasticDepth&quot;</span><span class="p">,</span>
    <span class="p">])</span>

    <span class="n">test_modules</span><span class="p">(</span><span class="n">nnx_module</span><span class="p">,</span> <span class="n">torch_module</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">36</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torchvision.models.maxvit</span><span class="w"> </span><span class="kn">import</span> <span class="n">MaxVitLayer</span> <span class="k">as</span> <span class="n">PyTorchMaxVitLayer</span>


<span class="n">stride</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">grid_size</span> <span class="o">=</span> <span class="n">_get_conv_output_shape</span><span class="p">((</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">),</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">norm_layer</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="n">torch_module</span> <span class="o">=</span> <span class="n">PyTorchMaxVitLayer</span><span class="p">(</span>
    <span class="mi">36</span><span class="p">,</span> <span class="mi">36</span><span class="p">,</span> <span class="n">squeeze_ratio</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">expansion_ratio</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">norm_layer</span><span class="o">=</span><span class="n">norm_layer</span><span class="p">,</span> <span class="n">activation_layer</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">,</span>
    <span class="n">head_dim</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">mlp_dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
    <span class="n">attention_dropout</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">p_stochastic_dropout</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
    <span class="n">partition_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">grid_size</span><span class="o">=</span><span class="n">grid_size</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">norm_layer</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.99</span><span class="p">)</span>
<span class="n">nnx_module</span> <span class="o">=</span> <span class="n">MaxVitLayer</span><span class="p">(</span>
    <span class="mi">36</span><span class="p">,</span> <span class="mi">36</span><span class="p">,</span> <span class="n">squeeze_ratio</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">expansion_ratio</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">norm_layer</span><span class="o">=</span><span class="n">norm_layer</span><span class="p">,</span> <span class="n">activation_layer</span><span class="o">=</span><span class="n">nnx</span><span class="o">.</span><span class="n">gelu</span><span class="p">,</span>
    <span class="n">head_dim</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">mlp_dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
    <span class="n">attention_dropout</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">p_stochastic_dropout</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
    <span class="n">partition_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">grid_size</span><span class="o">=</span><span class="n">grid_size</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">t2f</span><span class="o">.</span><span class="n">copy_module</span><span class="p">(</span><span class="n">torch_module</span><span class="p">,</span> <span class="n">nnx_module</span><span class="p">,</span> <span class="n">skip_modules</span><span class="o">=</span><span class="p">[</span>
    <span class="s2">&quot;WindowPartition&quot;</span><span class="p">,</span> <span class="s2">&quot;WindowDepartition&quot;</span><span class="p">,</span> <span class="s2">&quot;SwapAxes&quot;</span><span class="p">,</span> <span class="s2">&quot;StochasticDepth&quot;</span><span class="p">,</span>
<span class="p">])</span>


<span class="n">test_modules</span><span class="p">(</span><span class="n">nnx_module</span><span class="p">,</span> <span class="n">torch_module</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">36</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torchvision.models.maxvit</span><span class="w"> </span><span class="kn">import</span> <span class="n">MaxVitBlock</span> <span class="k">as</span> <span class="n">PyTorchMaxVitBlock</span>


<span class="n">norm_layer</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">torch_module</span> <span class="o">=</span> <span class="n">PyTorchMaxVitBlock</span><span class="p">(</span>
    <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">squeeze_ratio</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">expansion_ratio</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">norm_layer</span><span class="o">=</span><span class="n">norm_layer</span><span class="p">,</span> <span class="n">activation_layer</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">,</span>
    <span class="n">head_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">mlp_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">attention_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
    <span class="n">partition_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">input_grid_size</span><span class="o">=</span><span class="p">(</span><span class="mi">56</span><span class="p">,</span> <span class="mi">56</span><span class="p">),</span>
    <span class="n">n_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">p_stochastic</span><span class="o">=</span><span class="p">[</span><span class="mf">0.13333333333333333</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span>
<span class="p">)</span>

<span class="n">norm_layer</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.99</span><span class="p">)</span>
<span class="n">nnx_module</span> <span class="o">=</span> <span class="n">MaxVitBlock</span><span class="p">(</span>
    <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">squeeze_ratio</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">expansion_ratio</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">norm_layer</span><span class="o">=</span><span class="n">norm_layer</span><span class="p">,</span> <span class="n">activation_layer</span><span class="o">=</span><span class="n">nnx</span><span class="o">.</span><span class="n">gelu</span><span class="p">,</span>
    <span class="n">head_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">mlp_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">attention_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
    <span class="n">partition_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">input_grid_size</span><span class="o">=</span><span class="p">(</span><span class="mi">56</span><span class="p">,</span> <span class="mi">56</span><span class="p">),</span>
    <span class="n">n_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">p_stochastic</span><span class="o">=</span><span class="p">[</span><span class="mf">0.13333333333333333</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span>
<span class="p">)</span>

<span class="n">t2f</span><span class="o">.</span><span class="n">copy_module</span><span class="p">(</span><span class="n">torch_module</span><span class="p">,</span> <span class="n">nnx_module</span><span class="p">,</span> <span class="n">skip_modules</span><span class="o">=</span><span class="p">[</span>
    <span class="s2">&quot;WindowPartition&quot;</span><span class="p">,</span> <span class="s2">&quot;WindowDepartition&quot;</span><span class="p">,</span> <span class="s2">&quot;SwapAxes&quot;</span><span class="p">,</span> <span class="s2">&quot;StochasticDepth&quot;</span><span class="p">,</span>
<span class="p">])</span>

<span class="n">test_modules</span><span class="p">(</span><span class="n">nnx_module</span><span class="p">,</span> <span class="n">torch_module</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">56</span><span class="p">,</span> <span class="mi">56</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Finally, we can check the MaxVit implementation. Note that we raised the absolute tolerence to <code class="docutils literal notranslate"><span class="pre">1e-1</span></code> when comparing JAX output logits against PyTorch expected logits.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torchvision.models.maxvit</span><span class="w"> </span><span class="kn">import</span> <span class="n">MaxVit</span> <span class="k">as</span> <span class="n">PyTorchMaxVit</span>


<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">77</span><span class="p">)</span>


<span class="n">torch_module</span> <span class="o">=</span> <span class="n">PyTorchMaxVit</span><span class="p">(</span>
    <span class="n">input_size</span><span class="o">=</span><span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">),</span>
    <span class="n">stem_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="n">block_channels</span><span class="o">=</span><span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">],</span>
    <span class="n">block_layers</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
    <span class="n">head_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">stochastic_depth_prob</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">partition_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
    <span class="n">num_classes</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">nnx_module</span> <span class="o">=</span> <span class="n">MaxVit</span><span class="p">(</span>
    <span class="n">input_size</span><span class="o">=</span><span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">),</span>
    <span class="n">stem_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="n">block_channels</span><span class="o">=</span><span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">],</span>
    <span class="n">block_layers</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
    <span class="n">head_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">stochastic_depth_prob</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">partition_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
    <span class="n">num_classes</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">t2f</span><span class="o">.</span><span class="n">copy_module</span><span class="p">(</span><span class="n">torch_module</span><span class="p">,</span> <span class="n">nnx_module</span><span class="p">,</span> <span class="n">skip_modules</span><span class="o">=</span><span class="p">[</span>
    <span class="s2">&quot;WindowPartition&quot;</span><span class="p">,</span> <span class="s2">&quot;WindowDepartition&quot;</span><span class="p">,</span> <span class="s2">&quot;SwapAxes&quot;</span><span class="p">,</span> <span class="s2">&quot;StochasticDepth&quot;</span><span class="p">,</span>
<span class="p">])</span>


<span class="n">test_modules</span><span class="p">(</span><span class="n">nnx_module</span><span class="p">,</span> <span class="n">torch_module</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="check-flax-model">
<h3>Check Flax model<a class="headerlink" href="#check-flax-model" title="Link to this heading">#</a></h3>
<p>Let us now reuse trained weights from TorchVision’s MaxViT model to check output logits and the predictions on our example image:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torchvision.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">maxvit_t</span> <span class="k">as</span> <span class="n">pytorch_maxvit_t</span><span class="p">,</span> <span class="n">MaxVit_T_Weights</span>

<span class="n">torch_model</span> <span class="o">=</span> <span class="n">pytorch_maxvit_t</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">MaxVit_T_Weights</span><span class="o">.</span><span class="n">IMAGENET1K_V1</span><span class="p">)</span>
<span class="n">flax_model</span> <span class="o">=</span> <span class="n">maxvit_t</span><span class="p">()</span>

<span class="n">t2f</span> <span class="o">=</span> <span class="n">Torch2Flax</span><span class="p">()</span>
<span class="n">t2f</span><span class="o">.</span><span class="n">copy_module</span><span class="p">(</span><span class="n">torch_model</span><span class="p">,</span> <span class="n">flax_model</span><span class="p">,</span> <span class="n">skip_modules</span><span class="o">=</span><span class="p">[</span>
    <span class="s2">&quot;WindowPartition&quot;</span><span class="p">,</span> <span class="s2">&quot;WindowDepartition&quot;</span><span class="p">,</span> <span class="s2">&quot;SwapAxes&quot;</span><span class="p">,</span> <span class="s2">&quot;StochasticDepth&quot;</span><span class="p">,</span>
<span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">json</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchvision.io</span><span class="w"> </span><span class="kn">import</span> <span class="n">read_image</span>


<span class="n">preprocess</span> <span class="o">=</span> <span class="n">MaxVit_T_Weights</span><span class="o">.</span><span class="n">IMAGENET1K_V1</span><span class="o">.</span><span class="n">transforms</span><span class="p">()</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;imagenet_class_index.json&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">labels_file</span><span class="p">:</span>
  <span class="n">labels</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">labels_file</span><span class="p">)</span>


<span class="n">dog1</span> <span class="o">=</span> <span class="n">read_image</span><span class="p">(</span><span class="s2">&quot;dog1.jpg&quot;</span><span class="p">)</span>
<span class="n">tensor</span> <span class="o">=</span> <span class="n">preprocess</span><span class="p">(</span><span class="n">dog1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">torch_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
    <span class="n">torch_output</span> <span class="o">=</span> <span class="n">torch_model</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>

<span class="n">torch_class_id</span> <span class="o">=</span> <span class="n">torch_output</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

<span class="n">jax_array</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">jax</span><span class="o">.</span><span class="n">devices</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">flax_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">flax_output</span> <span class="o">=</span> <span class="n">flax_model</span><span class="p">(</span><span class="n">jax_array</span><span class="p">)</span>

<span class="n">flax_class_id</span> <span class="o">=</span> <span class="n">torch_output</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Prediction for the Dog:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;- PyTorch model result: </span><span class="si">{</span><span class="n">labels</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">torch_class_id</span><span class="p">)]</span><span class="si">}</span><span class="s2">, score: </span><span class="si">{</span><span class="n">torch_output</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">torch_class_id</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;- Flax model result: </span><span class="si">{</span><span class="n">labels</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">flax_class_id</span><span class="p">)]</span><span class="si">}</span><span class="s2">, score: </span><span class="si">{</span><span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">flax_output</span><span class="p">,</span><span class="w"> </span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">flax_class_id</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">labels</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">torch_class_id</span><span class="p">)]</span><span class="si">}</span><span class="se">\n</span><span class="s2">Score: </span><span class="si">{</span><span class="n">torch_output</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">class_id</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">dog1</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">labels</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">flax_class_id</span><span class="p">)]</span><span class="si">}</span><span class="se">\n</span><span class="s2">Score: </span><span class="si">{</span><span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">flax_output</span><span class="p">,</span><span class="w"> </span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">flax_class_id</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">dog1</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Prediction for the Dog:
- PyTorch model result: [&#39;n02113023&#39;, &#39;Pembroke&#39;], score: 0.7800846099853516
- Flax model result: [&#39;n02113023&#39;, &#39;Pembroke&#39;], score: 0.7799879908561707
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.image.AxesImage at 0x7ae7b2683fa0&gt;
</pre></div>
</div>
<img alt="_images/2cb2a16e45bf3a2a58979724bc530a852afcb90f4b2d3a0e46ff2e9390f4b45f.png" src="_images/2cb2a16e45bf3a2a58979724bc530a852afcb90f4b2d3a0e46ff2e9390f4b45f.png" />
</div>
</div>
<p>Let’s compute cosine distance between the logits:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">expected</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">torch_output</span><span class="p">)</span>

<span class="n">cosine_dist</span> <span class="o">=</span> <span class="p">(</span><span class="n">expected</span> <span class="o">*</span> <span class="n">flax_output</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">flax_output</span><span class="p">)</span> <span class="o">*</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">expected</span><span class="p">))</span>
<span class="n">cosine_dist</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Array(0.99999857, dtype=float32)
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="further-reading">
<h2>Further reading<a class="headerlink" href="#further-reading" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://flax.readthedocs.io/en/latest/examples/core_examples.html">Flax documentation: Core Examples</a></p></li>
<li><p><a class="reference external" href="https://jax-ai-stack.readthedocs.io/en/latest/getting_started.html">JAX AI Stack tutorials</a></p></li>
</ul>
</section>
</section>

<div class="section ablog__blog_comments">
   
</div>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="JAX_for_PyTorch_users.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">JAX for PyTorch users</p>
      </div>
    </a>
    <a class="right-next"
       href="JAX_for_LLM_pretraining.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Train a miniGPT language model with JAX</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#maxvit-pytorch-model-setup">MaxViT PyTorch model setup</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-s-architecture">Model’s architecture</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inference-on-data">Inference on data</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#port-maxvit-model-to-jax">Port MaxViT model to JAX</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conv2dnormactivation-implementation"><code class="docutils literal notranslate"><span class="pre">Conv2dNormActivation</span></code> implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#squeezeexcitation-implementation"><code class="docutils literal notranslate"><span class="pre">SqueezeExcitation</span></code> implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stochasticdepth-implementation"><code class="docutils literal notranslate"><span class="pre">StochasticDepth</span></code> implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mbconv-implementation"><code class="docutils literal notranslate"><span class="pre">MBConv</span></code> implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#relativepositionalmultiheadattention-implementation"><code class="docutils literal notranslate"><span class="pre">RelativePositionalMultiHeadAttention</span></code> implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#swapaxes-windowpartition-windowdepartition-implementations"><code class="docutils literal notranslate"><span class="pre">SwapAxes</span></code>, <code class="docutils literal notranslate"><span class="pre">WindowPartition</span></code>, <code class="docutils literal notranslate"><span class="pre">WindowDepartition</span></code> implementations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#partitionattentionlayer-implementation"><code class="docutils literal notranslate"><span class="pre">PartitionAttentionLayer</span></code> implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maxvitlayer-implementation"><code class="docutils literal notranslate"><span class="pre">MaxVitLayer</span></code> implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maxvitblock-implementation"><code class="docutils literal notranslate"><span class="pre">MaxVitBlock</span></code> implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maxvit-implementation"><code class="docutils literal notranslate"><span class="pre">MaxVit</span></code> implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#test-jax-implementation">Test JAX implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#check-flax-model">Check Flax model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-reading">Further reading</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By JAX team
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024, JAX team.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>